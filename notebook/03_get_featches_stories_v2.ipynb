{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55f6153a",
   "metadata": {},
   "source": [
    "# Восстановленный ноутбук (как в логах/kernel dump)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d946d474",
   "metadata": {},
   "source": [
    "## Import библиотек и модулей\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49fb6a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 08:28:58,972 | my_logger - INFO - ✅ PostgreSQL engine создан | /data/aturov/scoring/src/database.py:21\n",
      "2025-11-13 08:28:59,012 | my_logger - INFO - ✅ ClickHouse engine создан | /data/aturov/scoring/src/database.py:36\n",
      "2025-11-13 08:28:59,013 | my_logger - INFO - ✅ IPDR ClickHouse engine создан | /data/aturov/scoring/src/database.py:46\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "# --- Настройка путей и sys.path ---\n",
    "# Добавляем корневую директорию проекта в sys.path для импорта кастомных модулей\n",
    "PROJECT_ROOT = Path().cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "from src.config import config\n",
    "from src.logger import logger\n",
    "from src.database import clickhouse_engine, postgres_engine, ipdr_engine   \n",
    "import pandas as pd, pyarrow\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "# --- Настройка путей и sys.path ---\n",
    "# Добавляем корневую директорию проекта в sys.path для импорта кастомных модулей\n",
    "PROJECT_ROOT = Path().cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "from src.config import config\n",
    "from src.logger import logger\n",
    "from src.database import clickhouse_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb968cd",
   "metadata": {},
   "source": [
    "## Constant variables and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81bfa28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "TARGET_COL = 'target'\n",
    "# Key params (tune as needed)\n",
    "COUNT_WEEKS = 13                # implies history length N = 12 COUNT_WEEKS = [5, 13]\n",
    "OVERDUE_DAYS_MAX = 30           # example threshold for 'bad' target OVERDUE_DAYS_MAX = [10, 30]\n",
    "TOTAL_OVERDUE = 90               # example threshold for total overdue days/amount TOTAL_OVERDUE = [60, 90]\n",
    "DATE_END = \"2025-04-30\"         # cap for date_open (filtering); set as needed\n",
    "\n",
    "NAME_DATAFRAME_WEEKS = f'features_weeks_{COUNT_WEEKS - 1}_{OVERDUE_DAYS_MAX}_{TOTAL_OVERDUE}' # имя файла с признаками по неделям for LSTMs models\n",
    "NAME_DATAFRAME_TABLE = f'features_table_{COUNT_WEEKS - 1}_{OVERDUE_DAYS_MAX}_{TOTAL_OVERDUE}' # имя файла с табличными признаками\n",
    "NAME_DATAFRAME_BANKING = f'features_table_banking_{COUNT_WEEKS - 1}_{OVERDUE_DAYS_MAX}_{TOTAL_OVERDUE}' # имя файла с табличными признаками банковскими признаками"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6826b161",
   "metadata": {},
   "source": [
    "## Dowlnload данных из ClickHouse и Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24a80749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_query(engine, number_weeks):\n",
    "    \"\"\"\n",
    "    Функция для выполнения SQL-запроса к базе данных и получения данных в виде DataFrame.\n",
    "    Параметры:\n",
    "    - engine: SQLAlchemy engine для подключения к базе данных.\n",
    "    Возвращает:\n",
    "    - DataFrame с результатами запроса.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "        SELECT\n",
    "            w.*,\n",
    "            ce.id_request,\n",
    "            ce.phone_eldik, \n",
    "            ce.inn_eldik, \n",
    "            ce.id_credit, \n",
    "            ce.date_open, \n",
    "            ce.subscription_id, \n",
    "            ce.match_phone, \n",
    "            ce.match_inn, \n",
    "            ce.overdue_max, \n",
    "            ce.total_overdue,\n",
    "            ce.status\n",
    "        FROM (\n",
    "            SELECT *,\n",
    "                toDate(DT) AS dt_day\n",
    "            FROM DWH.dm_datamart_weekly \n",
    "            --WHERE STATUS not in ('Idle') -- исключаем статус Idle у него тот же cust_id \n",
    "        ) AS w\n",
    "        GLOBAL INNER JOIN (\n",
    "            SELECT \n",
    "                ce.id_request,\n",
    "                ce.phone_eldik, \n",
    "                ce.inn_eldik, \n",
    "                ce.id_credit, \n",
    "                ce.date_open, \n",
    "                toUInt64(ce.subscription_id) AS subscription_id,\n",
    "                ce.match_phone, \n",
    "                ce.match_inn, \n",
    "                IFNULL(ce.overdue_max, 0) AS overdue_max,\n",
    "                IFNULL(ce.total_overdue, 0) AS total_overdue,\n",
    "                ce.status\n",
    "            FROM data_science.credits_subs_eldik_clean AS ce\n",
    "        ) AS ce\n",
    "        ON w.SUBS_ID = ce.subscription_id  and w.DT = toStartOfWeek(ce.date_open - INTERVAL {number_weeks} WEEK - INTERVAL 1 DAY, 1) \n",
    "        -- DT - дата начала интервала в понедельник, в понедельник данные загружаются за предыдущую неделю и не всегда успевают попасть в витрину\n",
    "        WHERE 1=1\n",
    "        --and w.SUBS_ID =  1993788\n",
    "        \n",
    "    \"\"\"    \n",
    "    data = pd.read_sql(query, engine)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06620d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_features_parts = []\n",
    "#for number_week in range(1, COUNT_WEEKS):  # от 1 до 12 недель включительно\n",
    "#    print(f\"COUNT_WEEKS = {number_week}\")\n",
    "#    df_part = make_query(clickhouse_engine, number_weeks=number_week)\n",
    "#    if df_part is None or df_part.empty:\n",
    "#        logger.warning(f\"COUNT_WEEKS = {number_week}, пустой датафрейм, пропускаем\")\n",
    "#        continue\n",
    "#    df_part['count_weeks'] = number_week\n",
    "#    logger.info(f\"COUNT_WEEKS = {number_week}, shape = {df_part.shape}\")\n",
    "#    df_features_parts.append(df_part)\n",
    "#\n",
    "#df_features = pd.concat(df_features_parts, ignore_index=True) if df_features_parts else pd.DataFrame()\n",
    "#\n",
    "## приведение типов и сортировка\n",
    "#if 'DT' in df_features.columns:\n",
    "#    df_features['DT'] = pd.to_datetime(df_features['DT'], errors='coerce')\n",
    "#df_features.sort_values(['subscription_id', 'id_credit', 'DT'], ascending=[True, True, False], inplace=True)\n",
    "#df_features = df_features.query('count_weeks != 0')  # удаляем нулевую неделю (пересечение с датой открытия кредита)\n",
    "#df_features.to_parquet(config.environment.data_raw_path / f'aturov_features_{CURRENT_DATE}.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d7f9eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 08:28:59,547 | my_logger - INFO - Raw features shape: (248062, 110) | /tmp/ipykernel_1310143/2054622685.py:2\n",
      "/tmp/ipykernel_1310143/2054622685.py:34: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_features['days_from_dt_end_to_date_inactive'].fillna(-1, inplace=True)  # заполняем пропуски большим числом\n",
      "/tmp/ipykernel_1310143/2054622685.py:36: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_features['days_from_dt_end_to_date_lad'].fillna(-1, inplace=True)  # заполняем пропуски большим числом\n",
      "/tmp/ipykernel_1310143/2054622685.py:38: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_features['days_from_dt_end_to_price_change_date'].fillna(-1, inplace=True)  # заполняем пропуски большим числом\n",
      "/tmp/ipykernel_1310143/2054622685.py:40: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_features['days_from_dt_end_to_act_date'].fillna(-1, inplace=True)  # заполняем пропуски большим числом\n",
      "/tmp/ipykernel_1310143/2054622685.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_features['days_from_dt_end_to_date_abonka'].fillna(-1, inplace=True)  # заполняем пропуски большим числом\n",
      "/tmp/ipykernel_1310143/2054622685.py:44: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_features['days_from_dt_end_to_date_contract'].fillna(-1, inplace=True)  # заполняем пропуски большим числом\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(248062, 100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features = pd.read_parquet(config.environment.data_raw_path / 'aturov_features_2025-11-12.parquet')\n",
    "logger.info(f\"Raw features shape: {df_features.shape}\")\n",
    "#df_features = df_features.query(f'count_weeks <= {COUNT_WEEKS - 1}') # оставляем только нужное количество недель\n",
    "#logger.info(f\"Initial features shape: {df_features.shape}\")\n",
    "\n",
    "# formating\n",
    "df_features['TRANZ_FLAG'] = df_features['TRANZ_FLAG'].astype('boolean')\n",
    "df_features['FLAG_4G'] = df_features['FLAG_4G'].astype('boolean')\n",
    "df_features['ACTIVE_IND'] = df_features['ACTIVE_IND'].astype(str)\n",
    "df_features['FLAG_ABONKA'] = df_features['FLAG_ABONKA'].astype('boolean')\n",
    "df_features['MY_BEELINE_USER'] = df_features['MY_BEELINE_USER'].astype('boolean')\n",
    "df_features['BALANCE_USER'] = df_features['BALANCE_USER'].astype('boolean')\n",
    "df_features['MULTIPLAY'] = df_features['MULTIPLAY'].astype('boolean')\n",
    "df_features['M2M_FLAG'] = df_features['M2M_FLAG'].astype('boolean')\n",
    "df_features['match_inn'] = df_features['match_inn'].astype('boolean')\n",
    "df_features['match_phone'] = df_features['match_phone'].astype('boolean')\n",
    "# преобразование дат и создание новых признаков на основе разницы дат\n",
    "df_features['date_open'] = pd.to_datetime(df_features['date_open'], errors='coerce')\n",
    "df_features.dropna(subset=['date_open'], inplace=True)  # удаляем строки с некорректной датой открытия\n",
    "#df_features = df_features[df_features['date_open'] <= pd.to_datetime(DATE_END)]  # фильтрация по дате открытия\n",
    "#logger.info(f\"Filtered by date_open <= {DATE_END}, shape: {df_features.shape}\")\n",
    "df_features['DATE_INACTIVE'] = pd.to_datetime(df_features['DATE_INACTIVE'], errors='coerce')\n",
    "df_features['DATE_LAD'] = pd.to_datetime(df_features['DATE_LAD'], errors='coerce')\n",
    "df_features['PRICE_CHANGE_DATE'] = pd.to_datetime(df_features['PRICE_CHANGE_DATE'], errors='coerce')\n",
    "df_features['ACT_DATE'] = pd.to_datetime(df_features['ACT_DATE'], errors='coerce')\n",
    "df_features['DATE_ABONKA'] = pd.to_datetime(df_features['DATE_ABONKA'], errors='coerce')\n",
    "df_features['DATE_CONTRACT'] = pd.to_datetime(df_features['DATE_CONTRACT'], errors='coerce')\n",
    "df_features['DT'] = pd.to_datetime(df_features['DT'], errors='coerce')\n",
    "df_features['DT_END'] = (df_features['DT'] + pd.to_timedelta(7, unit='d')).dt.date # дата начала в понедельник влючительно и дата окончания в понедельник не включительно\n",
    "df_features['DT_END'] = pd.to_datetime(df_features['DT_END'], errors='coerce')\n",
    "\n",
    "# сразу посчитаем разницу дат \n",
    "df_features['days_from_dt_end_to_date_inactive'] = (df_features['DT_END'] - df_features['DATE_INACTIVE']).dt.days\n",
    "df_features['days_from_dt_end_to_date_inactive'].fillna(-1, inplace=True)  # заполняем пропуски большим числом\n",
    "df_features['days_from_dt_end_to_date_lad'] = (df_features['DT_END'] - df_features['DATE_LAD']).dt.days\n",
    "df_features['days_from_dt_end_to_date_lad'].fillna(-1, inplace=True)  # заполняем пропуски большим числом\n",
    "df_features['days_from_dt_end_to_price_change_date'] = (df_features['DT_END'] - df_features['PRICE_CHANGE_DATE']).dt.days\n",
    "df_features['days_from_dt_end_to_price_change_date'].fillna(-1, inplace=True)  # заполняем пропуски большим числом\n",
    "df_features['days_from_dt_end_to_act_date'] = (df_features['DT_END'] - df_features['ACT_DATE']).dt.days\n",
    "df_features['days_from_dt_end_to_act_date'].fillna(-1, inplace=True)  # заполняем пропуски большим числом\n",
    "df_features['days_from_dt_end_to_date_abonka'] = (df_features['DT_END'] - df_features['DATE_ABONKA']).dt.days\n",
    "df_features['days_from_dt_end_to_date_abonka'].fillna(-1, inplace=True)  # заполняем пропуски большим числом\n",
    "df_features['days_from_dt_end_to_date_contract'] = (df_features['DT_END'] - df_features['DATE_CONTRACT']).dt.days\n",
    "df_features['days_from_dt_end_to_date_contract'].fillna(-1, inplace=True)  # заполняем пропуски большим числом\n",
    "df_features.drop(columns=['DATE_INACTIVE', 'DATE_LAD', 'PRICE_CHANGE_DATE', 'ACT_DATE', 'DATE_ABONKA', 'DATE_CONTRACT', 'DT', 'DT_END'], inplace=True)\n",
    "# удалим те у кого много параметров\n",
    "#df_features.drop(columns=['CELL_MAX', 'CELL_ID', 'TAC', 'DEV_NAME'], inplace=True)\n",
    "df_features[['CELL_ID', 'CELL_MAX', 'DEV_NAME', 'TAC', 'PRICE_PLAN_RU']] = df_features[['CELL_ID', 'CELL_MAX', 'DEV_NAME', 'TAC', 'PRICE_PLAN_RU']].astype(str).fillna(\"missing\")\n",
    "\n",
    "\n",
    "# Создаем таргет: плохой клиент = 1, хороший = 0\n",
    "df_features['target'] = ((df_features['overdue_max'] >= OVERDUE_DAYS_MAX) | \n",
    "                        (df_features['total_overdue'] >= TOTAL_OVERDUE)| \n",
    "                        (df_features['status'].isin(['Отказано']))).astype(int)\n",
    "\n",
    "df_features.drop(columns=['IMEI', 'CTN','SUBS_ID', 'dt_day', 'FIRST_SIM', 'overdue_max', 'total_overdue', 'status'\n",
    "                          ,'USAGE_NUM_OFFNET_PAK', 'REVENUE_OFFNET_PAK'  # удалены из-за одного значения\n",
    "                          ], inplace=True)\n",
    "index_columns = ['id_request', 'subscription_id', 'phone_eldik', \n",
    "                 'inn_eldik', 'id_credit', 'date_open',\n",
    "                 ]\n",
    "sequential_columns = ['count_weeks']\n",
    "#df_features.set_index(index_columns, inplace=True)\n",
    "df_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a4c97d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CUST_LEVEL', 'STATUS', 'REGION_CELL', 'PRICE_PLAN', 'PRICE_PLAN_RU',\n",
       "       'PERIODICITY', 'SUBSCRIPTION_FEE', 'PREV_PRICE_PLAN', 'ORIG_PRICE_PLAN',\n",
       "       'BALANCE_END', 'REVENUE_ABONKA', 'USAGE_ABONKA_TP', 'TRANZ_FLAG',\n",
       "       'DAYS_WITHOUT_PAYMENT', 'TOTAL_RECHARGE', 'COUNT_RECHARGE', 'FLAG_4G',\n",
       "       'USAGE_NUM_OUT', 'USAGE_OUT_ONNET_VOICE', 'USAGE_OUT_OFFNET_VOICE',\n",
       "       'USAGE_OUT_CITY_VOICE', 'USAGE_OUT_INT_VOICE',\n",
       "       'USAGE_OUT_INT_VOICE_RUSSIA', 'USAGE_IN_ONNET_VOICE',\n",
       "       'USAGE_IN_OFFNET_VOICE', 'USAGE_VALUELESS_INTERNET', 'USAGE_INTERNET',\n",
       "       'USAGE_INTERNET_2G', 'USAGE_INTERNET_3G', 'USAGE_INTERNET_LTE',\n",
       "       'USAGE_INTERNET_3G_FREE', 'USAGE_INTERNET_LTE_FREE',\n",
       "       'USAGE_OUT_OFFNET_O_VOICE', 'USAGE_OUT_OFFNET_MEGACOM_VOICE',\n",
       "       'USAGE_IN_OFFNET_O_VOICE', 'USAGE_IN_OFFNET_MEGACOM_VOICE', 'COUNT_SMS',\n",
       "       'REVENUE_VOICE', 'REVENUE_VOICE_TO_SERVICE', 'REVENUE_OUT_ONNET_VOICE',\n",
       "       'REVENUE_OUT_OFFNET_VOICE', 'REVENUE_OUT_CITY_VOICE',\n",
       "       'REVENUE_OUT_INT_VOICE', 'REVENUE_INTERNET_PAYG',\n",
       "       'USAGE_INTERNET_NIGHT', 'USAGE_NUM_INTERNET_PAK',\n",
       "       'REVENUE_INTERNET_PAK', 'INTERCONNECT_MN_IN', 'INTERCONNECT_MN_OUT',\n",
       "       'INTERCONNECT_LOC_IN', 'INTERCONNECT_LOC_OUT',\n",
       "       'REVENUE_TOTAL_INTERCONNECT', 'GM', 'IVR_LANG', 'TAC', 'CELL_ID',\n",
       "       'DEV_NAME', 'DEV_TYPE', 'FLAG_DEVICE_4G', 'OS_NAME', 'REVENUE_TOTAL',\n",
       "       'OTHER_CHARGES', 'ACTIVE_IND', 'USAGE_OUT_FREE_OFFNET_VOICE',\n",
       "       'REVENUE_DAILY_ABONKA', 'USAGE_DAILY_ABONKA', 'REGION',\n",
       "       'REVENUE_ROUMING', 'USAGE_NUM_INC', 'REVENUE_OFFNET_O_VOICE',\n",
       "       'REVENUE_OFFNET_MEGACOM_VOICE', 'CELL_MAX', 'ROLY_VOICE_CHARGE',\n",
       "       'ROLY_DATA_CHARGE', 'ROLY_GLOBAL', 'FLAG_ABONKA', 'TOTAL_MOU',\n",
       "       'MY_BEELINE_USER', 'BALANCE_USER', 'MULTIPLAY', 'LIFETIME_TOTAL',\n",
       "       'M2M_FLAG', 'GENDER', 'AGE', 'id_request', 'phone_eldik', 'inn_eldik',\n",
       "       'id_credit', 'date_open', 'subscription_id', 'match_phone', 'match_inn',\n",
       "       'count_weeks', 'days_from_dt_end_to_date_inactive',\n",
       "       'days_from_dt_end_to_date_lad', 'days_from_dt_end_to_price_change_date',\n",
       "       'days_from_dt_end_to_act_date', 'days_from_dt_end_to_date_abonka',\n",
       "       'days_from_dt_end_to_date_contract', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca694b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dt</td>\n",
       "      <td>Дата начала расчётного периода</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ctn</td>\n",
       "      <td>CTN (номер) абонента</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subs_id</td>\n",
       "      <td>Ключ абонента</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cust_level</td>\n",
       "      <td>Тип клиента (B2B, B2C, Employee, etc)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>status</td>\n",
       "      <td>Статус абонента (активный, блокированный, прио...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>multiplay</td>\n",
       "      <td>Входит в мультиплеи (юзер одного из приложений...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>lifetime_total</td>\n",
       "      <td>Срок жизни абонента в нашей сети (дни)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>m2m_flag</td>\n",
       "      <td>Флаг M2M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>gender</td>\n",
       "      <td>Пол абонента по модели дата сайнтистов</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>age</td>\n",
       "      <td>Возраст абонента по модели дата сайнтистов</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          features                                        description\n",
       "0               dt                     Дата начала расчётного периода\n",
       "1              ctn                              CTN (номер) абонента \n",
       "2          subs_id                                      Ключ абонента\n",
       "3       cust_level              Тип клиента (B2B, B2C, Employee, etc)\n",
       "4           status  Статус абонента (активный, блокированный, прио...\n",
       "..             ...                                                ...\n",
       "93       multiplay  Входит в мультиплеи (юзер одного из приложений...\n",
       "94  lifetime_total             Срок жизни абонента в нашей сети (дни)\n",
       "95        m2m_flag                                           Флаг M2M\n",
       "96          gender             Пол абонента по модели дата сайнтистов\n",
       "97             age         Возраст абонента по модели дата сайнтистов\n",
       "\n",
       "[98 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe_features = pd.read_excel('/data/aturov/scoring/docs/Переменные_витрины_new_(3).xlsx')\n",
    "describe_features.columns = ['features', 'description']\n",
    "describe_features['features'] = describe_features['features'].str.lower()\n",
    "describe_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2683d300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    12785\n",
       "1     7328\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_stats = df_features.copy().query('count_weeks == 12')\n",
    "target_stats = target_stats['target'].value_counts().sort_index()\n",
    "target_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd39aa51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id_request',\n",
       " 'subscription_id',\n",
       " 'phone_eldik',\n",
       " 'inn_eldik',\n",
       " 'id_credit',\n",
       " 'date_open']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91f97de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only_12_weeks = df_features.groupby(index_columns)['count_weeks'].count().sort_values().reset_index()\n",
    "#\n",
    "#only_12_weeks = only_12_weeks[only_12_weeks['count_weeks'] == COUNT_WEEKS - 1]\n",
    "#only_12_weeks.set_index(index_columns, inplace=True)\n",
    "#only_12_weeks.drop(columns=['count_weeks'], inplace=True)\n",
    "#only_12_weeks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74c8013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.set_index(index_columns, inplace=True)\n",
    "#df_features = df_features.join(only_12_weeks, how='inner', on=index_columns)\n",
    "#df_features.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a542d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4da56f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Dtypes & index columns ===\n",
    "datetime_cols = df_features.select_dtypes(include=['datetime64[ns]', 'datetimetz']).columns.tolist()\n",
    "categorical_cols = df_features.select_dtypes(include=['object','category','boolean']).columns.tolist()\n",
    "numeric_cols = df_features.select_dtypes(include=['float64','int64','float32','int32']).columns.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00713bb5",
   "metadata": {},
   "source": [
    "### Сохраним для дальенйшего обучения weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92a34191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 08:29:02,479 | my_logger - INFO - Features saved to /data/aturov/scoring/data/processed/features_weeks_12_30_90_2025-11-13.parquet | /tmp/ipykernel_1310143/1195538844.py:2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_features.to_parquet(config.environment.data_processed_path / f'{NAME_DATAFRAME_WEEKS}_{CURRENT_DATE}.parquet', index=True)\n",
    "logger.info(f\"Features saved to {config.environment.data_processed_path / f'{NAME_DATAFRAME_WEEKS}_{CURRENT_DATE}.parquet'}\")\n",
    "# Сохранение категориальных признаков и числовых в json\n",
    "\n",
    "with open(config.environment.data_processed_path / f'{NAME_DATAFRAME_WEEKS}_num_columns_{CURRENT_DATE}.json', 'w') as f:\n",
    "    json.dump(numeric_cols, f)\n",
    "with open(config.environment.data_processed_path / f'{NAME_DATAFRAME_WEEKS}_cat_columns_{CURRENT_DATE}.json', 'w') as f:\n",
    "    json.dump(categorical_cols, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a11b5c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(248062, 94)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ebcbfb",
   "metadata": {},
   "source": [
    "### Переведем недели в фичи для обычных моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2966d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20852, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_cols = [col for col in numeric_cols if col not in (sequential_columns[0], TARGET_COL)]\n",
    "categorical_cols = [col for col in categorical_cols if col not in (sequential_columns[0], TARGET_COL)]\n",
    "\n",
    "tgt = df_features.reset_index(drop=False).drop_duplicates(subset=index_columns)[index_columns + [TARGET_COL]]\n",
    "tgt = tgt.set_index(index_columns)\n",
    "tgt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bdf9958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_weeks(df: pd.DataFrame, id_cols, seq_col, numeric_cols, categorical_cols, COUNT_WEEKS: int):\n",
    "    \"\"\"Возвращает num_wide и cat_wide (индекс = id_cols).\"\"\"\n",
    "    weeks = list(range(1, COUNT_WEEKS))\n",
    "    df_reset = df.reset_index(drop=False)\n",
    "\n",
    "    pnum = df_reset.pivot_table(index=id_cols, columns=seq_col, values=numeric_cols, aggfunc='first')\n",
    "    if isinstance(pnum.columns, pd.MultiIndex):\n",
    "        pnum.columns = [f\"{feat}_w{int(wk) if str(wk).replace('.0','').isdigit() else wk}\" for feat, wk in pnum.columns.to_list()]\n",
    "    else:\n",
    "        pnum.columns = [str(c) for c in pnum.columns.to_list()]\n",
    "    num_wide_cols = [f\"{feat}_w{wk}\" for feat in numeric_cols for wk in weeks if f\"{feat}_w{wk}\" in pnum.columns]\n",
    "    num_wide = pnum.reindex(columns=num_wide_cols)\n",
    "\n",
    "    pcat = df_reset.pivot_table(index=id_cols, columns=seq_col, values=categorical_cols, aggfunc='first')\n",
    "    if isinstance(pcat.columns, pd.MultiIndex):\n",
    "        pcat.columns = [f\"{feat}_w{int(wk) if str(wk).replace('.0','').isdigit() else wk}\" for feat, wk in pcat.columns.to_list()]\n",
    "    else:\n",
    "        pcat.columns = [str(c) for c in pcat.columns.to_list()]\n",
    "    cat_wide_cols = [f\"{feat}_w{wk}\" for feat in categorical_cols for wk in weeks if f\"{feat}_w{wk}\" in pcat.columns]\n",
    "    cat_wide = pcat.reindex(columns=cat_wide_cols)\n",
    "\n",
    "    return num_wide, cat_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc15816a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20852, 756), (20852, 348))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# получаем wide-таблицы\n",
    "num_wide, cat_wide = pivot_weeks(df_features.reset_index(drop=False), index_columns, sequential_columns[0],\n",
    "                                 numeric_cols, categorical_cols, COUNT_WEEKS)\n",
    "num_wide.shape, cat_wide.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f79f8f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cat_features_from_wide(cat_wide: pd.DataFrame, categorical_cols, COUNT_WEEKS: int, changes_as_bool: bool = False):\n",
    "    \"\"\"Для каждого categorical_cols возвращает first,last,mode,changes (DataFrame).\n",
    "    Параметры:\n",
    "      changes_as_bool — если True, колонка {feat}_changes будет булевой (True если было хотя бы одно изменение),\n",
    "                        иначе целочисленная (количество смен).\n",
    "    Возвращает: (cat_tab, cat_tab_columns, cat_wide_columns, num_feats)\n",
    "    \"\"\"\n",
    "    weeks = list(range(1, COUNT_WEEKS))\n",
    "    cat_feats = {}\n",
    "    num_feats = []\n",
    "    for feat in categorical_cols:\n",
    "        cols = [f\"{feat}_w{wk}\" for wk in weeks if f\"{feat}_w{wk}\" in cat_wide.columns]\n",
    "\n",
    "        if cols:\n",
    "            # берём значения по неделям, заменяем NA на 'missing' и приводим к str\n",
    "            C = cat_wide[cols].fillna(\"missing\").astype(str)\n",
    "            first_ser = C.get(f\"{feat}_w1\", C.iloc[:, 0]).astype(str)\n",
    "            last_ser = C.get(f\"{feat}_w{COUNT_WEEKS-1}\", C.iloc[:, -1]).astype(str)\n",
    "            # mode: безопасно получить первый модальный, иначе fallback на первый столбец\n",
    "            try:\n",
    "                mode_ser = C.mode(axis=1).iloc[:, 0].astype(str)\n",
    "            except Exception:\n",
    "                mode_ser = C.iloc[:, 0].astype(str)\n",
    "\n",
    "            # changes: подсчёт смен между соседними неделями\n",
    "            arr = C.to_numpy(dtype=object)\n",
    "            if arr.shape[1] > 1:\n",
    "                changes_cnt = (arr[:, 1:] != arr[:, :-1]).sum(axis=1).astype(np.int32)\n",
    "            else:\n",
    "                changes_cnt = np.zeros(C.shape[0], dtype=np.int32)\n",
    "\n",
    "            if changes_as_bool:\n",
    "                changes_ser = pd.Series(changes_cnt > 0, index=C.index, dtype=bool)\n",
    "            else:\n",
    "                changes_ser = pd.Series(changes_cnt, index=C.index, dtype=np.int32)\n",
    "\n",
    "            cat_feats[f\"{feat}_first\"] = first_ser\n",
    "            cat_feats[f\"{feat}_last\"] = last_ser\n",
    "            cat_feats[f\"{feat}_mode\"] = mode_ser\n",
    "            cat_feats[f\"{feat}_changes\"] = changes_ser\n",
    "            num_feats.append(f\"{feat}_changes\")\n",
    "        else:\n",
    "            # нет wide-столбцов для признака — ставим заглушки\n",
    "            idx = cat_wide.index\n",
    "            cat_feats[f\"{feat}_first\"] = pd.Series(\"missing\", index=idx, dtype=object)\n",
    "            cat_feats[f\"{feat}_last\"] = pd.Series(\"missing\", index=idx, dtype=object)\n",
    "            cat_feats[f\"{feat}_mode\"] = pd.Series(\"missing\", index=idx, dtype=object)\n",
    "            if changes_as_bool:\n",
    "                cat_feats[f\"{feat}_changes\"] = pd.Series(False, index=idx, dtype=bool)\n",
    "            else:\n",
    "                cat_feats[f\"{feat}_changes\"] = pd.Series(0, index=idx, dtype=np.int32)\n",
    "            num_feats.append(f\"{feat}_changes\")\n",
    "\n",
    "    cat_tab = pd.DataFrame(cat_feats, index=cat_wide.index)\n",
    "    cat_tab_columns = [c for c in cat_tab.columns.tolist() if c not in num_feats]\n",
    "    # Возвращаем: (таблица, список категориальных итоговых признаков, список колонок wide, список числовых _changes)\n",
    "    return cat_tab, cat_tab_columns, cat_wide.columns.tolist(), num_feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16acb5d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid value 'missing' for dtype 'boolean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/data/aturov/scoring/.venv/lib/python3.10/site-packages/pandas/core/internals/blocks.py:2401\u001b[0m, in \u001b[0;36mExtensionBlock.fillna\u001b[0;34m(self, value, limit, inplace, downcast, using_cow, already_warned)\u001b[0m\n\u001b[1;32m   2400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2401\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[1;32m   2403\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2404\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   2405\u001b[0m     \u001b[38;5;66;03m# 3rd party EA that has not implemented copy keyword yet\u001b[39;00m\n",
      "File \u001b[0;32m/data/aturov/scoring/.venv/lib/python3.10/site-packages/pandas/core/arrays/masked.py:267\u001b[0m, in \u001b[0;36mBaseMaskedArray.fillna\u001b[0;34m(self, value, method, limit, copy)\u001b[0m\n\u001b[1;32m    266\u001b[0m             new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m[:]\n\u001b[0;32m--> 267\u001b[0m         \u001b[43mnew_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/data/aturov/scoring/.venv/lib/python3.10/site-packages/pandas/core/arrays/masked.py:315\u001b[0m, in \u001b[0;36mBaseMaskedArray.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 315\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_setitem_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[key] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[0;32m/data/aturov/scoring/.venv/lib/python3.10/site-packages/pandas/core/arrays/masked.py:306\u001b[0m, in \u001b[0;36mBaseMaskedArray._validate_setitem_value\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;66;03m# TODO: unsigned checks\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# Note: without the \"str\" here, the f-string rendering raises in\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m#  py38 builds.\u001b[39;00m\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid value \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for dtype \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid value 'missing' for dtype 'boolean'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cat_tab, cat_tab_columns, cat_wide_columns, num_feats \u001b[38;5;241m=\u001b[39m \u001b[43mmake_cat_features_from_wide\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcat_wide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCOUNT_WEEKS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated categorical features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcat_tab_columns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCategorical wide columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcat_wide_columns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 16\u001b[0m, in \u001b[0;36mmake_cat_features_from_wide\u001b[0;34m(cat_wide, categorical_cols, COUNT_WEEKS, changes_as_bool)\u001b[0m\n\u001b[1;32m     12\u001b[0m cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeat\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_w\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m wk \u001b[38;5;129;01min\u001b[39;00m weeks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeat\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_w\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m cat_wide\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cols:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# берём значения по неделям, заменяем NA на 'missing' и приводим к str\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     C \u001b[38;5;241m=\u001b[39m \u001b[43mcat_wide\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmissing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m     17\u001b[0m     first_ser \u001b[38;5;241m=\u001b[39m C\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeat\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_w1\u001b[39m\u001b[38;5;124m\"\u001b[39m, C\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m     18\u001b[0m     last_ser \u001b[38;5;241m=\u001b[39m C\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeat\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_w\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCOUNT_WEEKS\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, C\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n",
      "File \u001b[0;32m/data/aturov/scoring/.venv/lib/python3.10/site-packages/pandas/core/generic.py:7457\u001b[0m, in \u001b[0;36mNDFrame.fillna\u001b[0;34m(self, value, method, axis, inplace, limit, downcast)\u001b[0m\n\u001b[1;32m   7455\u001b[0m         new_data \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m_mgr\n\u001b[1;32m   7456\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 7457\u001b[0m         new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   7458\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdowncast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdowncast\u001b[49m\n\u001b[1;32m   7459\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   7460\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, ABCDataFrame) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   7461\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhere(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotna(), value)\u001b[38;5;241m.\u001b[39m_mgr\n",
      "File \u001b[0;32m/data/aturov/scoring/.venv/lib/python3.10/site-packages/pandas/core/internals/base.py:186\u001b[0m, in \u001b[0;36mDataManager.fillna\u001b[0;34m(self, value, limit, inplace, downcast)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Do this validation even if we go through one of the no-op paths\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     limit \u001b[38;5;241m=\u001b[39m libalgos\u001b[38;5;241m.\u001b[39mvalidate_limit(\u001b[38;5;28;01mNone\u001b[39;00m, limit\u001b[38;5;241m=\u001b[39mlimit)\n\u001b[0;32m--> 186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_with_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfillna\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdowncast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdowncast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43malready_warned\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_AlreadyWarned\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/aturov/scoring/.venv/lib/python3.10/site-packages/pandas/core/internals/managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m/data/aturov/scoring/.venv/lib/python3.10/site-packages/pandas/core/internals/blocks.py:2407\u001b[0m, in \u001b[0;36mExtensionBlock.fillna\u001b[0;34m(self, value, limit, inplace, downcast, using_cow, already_warned)\u001b[0m\n\u001b[1;32m   2404\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   2405\u001b[0m     \u001b[38;5;66;03m# 3rd party EA that has not implemented copy keyword yet\u001b[39;00m\n\u001b[1;32m   2406\u001b[0m     refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2407\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2408\u001b[0m     \u001b[38;5;66;03m# issue the warning *after* retrying, in case the TypeError\u001b[39;00m\n\u001b[1;32m   2409\u001b[0m     \u001b[38;5;66;03m#  was caused by an invalid fill_value\u001b[39;00m\n\u001b[1;32m   2410\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2411\u001b[0m         \u001b[38;5;66;03m# GH#53278\u001b[39;00m\n\u001b[1;32m   2412\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtensionArray.fillna added a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcopy\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keyword in pandas \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2418\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m   2419\u001b[0m     )\n",
      "File \u001b[0;32m/data/aturov/scoring/.venv/lib/python3.10/site-packages/pandas/core/arrays/masked.py:267\u001b[0m, in \u001b[0;36mBaseMaskedArray.fillna\u001b[0;34m(self, value, method, limit, copy)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m             new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m[:]\n\u001b[0;32m--> 267\u001b[0m         \u001b[43mnew_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m copy:\n",
      "File \u001b[0;32m/data/aturov/scoring/.venv/lib/python3.10/site-packages/pandas/core/arrays/masked.py:315\u001b[0m, in \u001b[0;36mBaseMaskedArray.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mask[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 315\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_setitem_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mask[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/data/aturov/scoring/.venv/lib/python3.10/site-packages/pandas/core/arrays/masked.py:306\u001b[0m, in \u001b[0;36mBaseMaskedArray._validate_setitem_value\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;66;03m# TODO: unsigned checks\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# Note: without the \"str\" here, the f-string rendering raises in\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m#  py38 builds.\u001b[39;00m\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid value \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for dtype \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid value 'missing' for dtype 'boolean'"
     ]
    }
   ],
   "source": [
    "cat_tab, cat_tab_columns, cat_wide_columns, num_feats = make_cat_features_from_wide(cat_wide, categorical_cols, COUNT_WEEKS)\n",
    "logger.info(f\"Created categorical features: {cat_tab_columns}\")\n",
    "logger.info(f\"Categorical wide columns: {cat_wide_columns}\")\n",
    "logger.info(f\"Categorical features table shape: {cat_tab.shape}\")\n",
    "logger.info(f\"Numerical features: {num_feats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ca5d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_num_features_from_wide(num_wide: pd.DataFrame, numeric_cols, COUNT_WEEKS: int):\n",
    "    \"\"\"Для каждого numeric_cols возвращает first,last,mean,std,trend,min,max,last_nonnull_count.\"\"\"\n",
    "    num_aggs = {}\n",
    "    weeks = list(range(1, COUNT_WEEKS))\n",
    "    for feat in numeric_cols:\n",
    "        cols = [c for c in num_wide.columns if c.startswith(f\"{feat}_w\")]\n",
    "        if len(cols) == 0:\n",
    "            idx = num_wide.index\n",
    "            num_aggs[f\"{feat}_first\"] = pd.Series(np.nan, index=idx)\n",
    "            num_aggs[f\"{feat}_last\"]  = pd.Series(np.nan, index=idx)\n",
    "            num_aggs[f\"{feat}_mean\"]  = pd.Series(np.nan, index=idx)\n",
    "            num_aggs[f\"{feat}_std\"]   = pd.Series(np.nan, index=idx)\n",
    "            num_aggs[f\"{feat}_trend\"] = pd.Series(0.0, index=idx)\n",
    "            num_aggs[f\"{feat}_min\"]   = pd.Series(np.nan, index=idx)\n",
    "            num_aggs[f\"{feat}_max\"]   = pd.Series(np.nan, index=idx)\n",
    "            num_aggs[f\"{feat}_last_nonnull_count\"] = pd.Series(0, index=idx, dtype=int)\n",
    "            continue\n",
    "\n",
    "        mat = num_wide[cols].to_numpy(dtype=float)\n",
    "        mask = ~np.isnan(mat)\n",
    "        counts = mask.sum(axis=1)\n",
    "\n",
    "        # first = week1 (most distant) fallback -> first available\n",
    "        first_col = f\"{feat}_w1\"\n",
    "        last_col  = f\"{feat}_w{COUNT_WEEKS-1}\"\n",
    "        s_first = num_wide[first_col] if first_col in num_wide.columns else num_wide[cols].iloc[:, 0]\n",
    "        s_last  = num_wide[last_col]  if last_col  in num_wide.columns else num_wide[cols].iloc[:, -1]\n",
    "\n",
    "        num_aggs[f\"{feat}_first\"] = s_first.astype(float)\n",
    "        num_aggs[f\"{feat}_last\"]  = s_last.astype(float)\n",
    "\n",
    "        num_aggs[f\"{feat}_mean\"] = pd.Series(np.nanmean(mat, axis=1), index=num_wide.index)\n",
    "        num_aggs[f\"{feat}_std\"]  = pd.Series(np.nanstd(mat, axis=1), index=num_wide.index)\n",
    "        num_aggs[f\"{feat}_min\"]  = pd.Series(np.nanmin(mat, axis=1), index=num_wide.index)\n",
    "        num_aggs[f\"{feat}_max\"]  = pd.Series(np.nanmax(mat, axis=1), index=num_wide.index)\n",
    "        num_aggs[f\"{feat}_last_nonnull_count\"] = pd.Series(counts, index=num_wide.index)\n",
    "\n",
    "        # trend: векторно (линейный slope) по реальным номерам недель в cols\n",
    "        present_wks = np.array([int(c.rsplit('_w', 1)[1]) for c in cols], dtype=float)\n",
    "        mean_y = np.nanmean(mat, axis=1)\n",
    "        weighted_w = (mask * present_wks).sum(axis=1)\n",
    "        mean_w = np.where(counts > 0, weighted_w / np.maximum(counts, 1), 0.0)\n",
    "        num = ((present_wks - mean_w[:, None]) * (mat - mean_y[:, None])) * mask\n",
    "        numerator = np.nansum(num, axis=1)\n",
    "        denom = np.sum(((present_wks - mean_w[:, None]) ** 2) * mask, axis=1)\n",
    "        slope = np.where(denom > 0, numerator / denom, 0.0)\n",
    "        slope[counts < 2] = 0.0\n",
    "        num_aggs[f\"{feat}_trend\"] = pd.Series(slope, index=num_wide.index)\n",
    "\n",
    "    num_tab = pd.DataFrame(num_aggs, index=num_wide.index)\n",
    "    return num_tab, num_tab.columns.tolist(), num_wide.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8d1156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3308355/1963821778.py:32: RuntimeWarning: Mean of empty slice\n",
      "  num_aggs[f\"{feat}_mean\"] = pd.Series(np.nanmean(mat, axis=1), index=num_wide.index)\n",
      "/data/aturov/scoring/.venv/lib/python3.10/site-packages/numpy/lib/_nanfunctions_impl.py:2019: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/tmp/ipykernel_3308355/1963821778.py:34: RuntimeWarning: All-NaN slice encountered\n",
      "  num_aggs[f\"{feat}_min\"]  = pd.Series(np.nanmin(mat, axis=1), index=num_wide.index)\n",
      "/tmp/ipykernel_3308355/1963821778.py:35: RuntimeWarning: All-NaN slice encountered\n",
      "  num_aggs[f\"{feat}_max\"]  = pd.Series(np.nanmax(mat, axis=1), index=num_wide.index)\n",
      "/tmp/ipykernel_3308355/1963821778.py:40: RuntimeWarning: Mean of empty slice\n",
      "  mean_y = np.nanmean(mat, axis=1)\n",
      "/tmp/ipykernel_3308355/1963821778.py:46: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = np.where(denom > 0, numerator / denom, 0.0)\n",
      "/tmp/ipykernel_3308355/1963821778.py:32: RuntimeWarning: Mean of empty slice\n",
      "  num_aggs[f\"{feat}_mean\"] = pd.Series(np.nanmean(mat, axis=1), index=num_wide.index)\n",
      "/data/aturov/scoring/.venv/lib/python3.10/site-packages/numpy/lib/_nanfunctions_impl.py:2019: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/tmp/ipykernel_3308355/1963821778.py:34: RuntimeWarning: All-NaN slice encountered\n",
      "  num_aggs[f\"{feat}_min\"]  = pd.Series(np.nanmin(mat, axis=1), index=num_wide.index)\n",
      "/tmp/ipykernel_3308355/1963821778.py:35: RuntimeWarning: All-NaN slice encountered\n",
      "  num_aggs[f\"{feat}_max\"]  = pd.Series(np.nanmax(mat, axis=1), index=num_wide.index)\n",
      "/tmp/ipykernel_3308355/1963821778.py:40: RuntimeWarning: Mean of empty slice\n",
      "  mean_y = np.nanmean(mat, axis=1)\n",
      "/tmp/ipykernel_3308355/1963821778.py:46: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = np.where(denom > 0, numerator / denom, 0.0)\n",
      "2025-10-30 15:56:55,359 | my_logger - INFO - Created numeric features: ['SUBSCRIPTION_FEE_first', 'SUBSCRIPTION_FEE_last', 'SUBSCRIPTION_FEE_mean', 'SUBSCRIPTION_FEE_std', 'SUBSCRIPTION_FEE_min', 'SUBSCRIPTION_FEE_max', 'SUBSCRIPTION_FEE_last_nonnull_count', 'SUBSCRIPTION_FEE_trend', 'BALANCE_END_first', 'BALANCE_END_last', 'BALANCE_END_mean', 'BALANCE_END_std', 'BALANCE_END_min', 'BALANCE_END_max', 'BALANCE_END_last_nonnull_count', 'BALANCE_END_trend', 'REVENUE_ABONKA_first', 'REVENUE_ABONKA_last', 'REVENUE_ABONKA_mean', 'REVENUE_ABONKA_std', 'REVENUE_ABONKA_min', 'REVENUE_ABONKA_max', 'REVENUE_ABONKA_last_nonnull_count', 'REVENUE_ABONKA_trend', 'USAGE_ABONKA_TP_first', 'USAGE_ABONKA_TP_last', 'USAGE_ABONKA_TP_mean', 'USAGE_ABONKA_TP_std', 'USAGE_ABONKA_TP_min', 'USAGE_ABONKA_TP_max', 'USAGE_ABONKA_TP_last_nonnull_count', 'USAGE_ABONKA_TP_trend', 'DAYS_WITHOUT_PAYMENT_first', 'DAYS_WITHOUT_PAYMENT_last', 'DAYS_WITHOUT_PAYMENT_mean', 'DAYS_WITHOUT_PAYMENT_std', 'DAYS_WITHOUT_PAYMENT_min', 'DAYS_WITHOUT_PAYMENT_max', 'DAYS_WITHOUT_PAYMENT_last_nonnull_count', 'DAYS_WITHOUT_PAYMENT_trend', 'TOTAL_RECHARGE_first', 'TOTAL_RECHARGE_last', 'TOTAL_RECHARGE_mean', 'TOTAL_RECHARGE_std', 'TOTAL_RECHARGE_min', 'TOTAL_RECHARGE_max', 'TOTAL_RECHARGE_last_nonnull_count', 'TOTAL_RECHARGE_trend', 'COUNT_RECHARGE_first', 'COUNT_RECHARGE_last', 'COUNT_RECHARGE_mean', 'COUNT_RECHARGE_std', 'COUNT_RECHARGE_min', 'COUNT_RECHARGE_max', 'COUNT_RECHARGE_last_nonnull_count', 'COUNT_RECHARGE_trend', 'USAGE_NUM_OUT_first', 'USAGE_NUM_OUT_last', 'USAGE_NUM_OUT_mean', 'USAGE_NUM_OUT_std', 'USAGE_NUM_OUT_min', 'USAGE_NUM_OUT_max', 'USAGE_NUM_OUT_last_nonnull_count', 'USAGE_NUM_OUT_trend', 'USAGE_OUT_ONNET_VOICE_first', 'USAGE_OUT_ONNET_VOICE_last', 'USAGE_OUT_ONNET_VOICE_mean', 'USAGE_OUT_ONNET_VOICE_std', 'USAGE_OUT_ONNET_VOICE_min', 'USAGE_OUT_ONNET_VOICE_max', 'USAGE_OUT_ONNET_VOICE_last_nonnull_count', 'USAGE_OUT_ONNET_VOICE_trend', 'USAGE_OUT_OFFNET_VOICE_first', 'USAGE_OUT_OFFNET_VOICE_last', 'USAGE_OUT_OFFNET_VOICE_mean', 'USAGE_OUT_OFFNET_VOICE_std', 'USAGE_OUT_OFFNET_VOICE_min', 'USAGE_OUT_OFFNET_VOICE_max', 'USAGE_OUT_OFFNET_VOICE_last_nonnull_count', 'USAGE_OUT_OFFNET_VOICE_trend', 'USAGE_OUT_CITY_VOICE_first', 'USAGE_OUT_CITY_VOICE_last', 'USAGE_OUT_CITY_VOICE_mean', 'USAGE_OUT_CITY_VOICE_std', 'USAGE_OUT_CITY_VOICE_min', 'USAGE_OUT_CITY_VOICE_max', 'USAGE_OUT_CITY_VOICE_last_nonnull_count', 'USAGE_OUT_CITY_VOICE_trend', 'USAGE_OUT_INT_VOICE_first', 'USAGE_OUT_INT_VOICE_last', 'USAGE_OUT_INT_VOICE_mean', 'USAGE_OUT_INT_VOICE_std', 'USAGE_OUT_INT_VOICE_min', 'USAGE_OUT_INT_VOICE_max', 'USAGE_OUT_INT_VOICE_last_nonnull_count', 'USAGE_OUT_INT_VOICE_trend', 'USAGE_OUT_INT_VOICE_RUSSIA_first', 'USAGE_OUT_INT_VOICE_RUSSIA_last', 'USAGE_OUT_INT_VOICE_RUSSIA_mean', 'USAGE_OUT_INT_VOICE_RUSSIA_std', 'USAGE_OUT_INT_VOICE_RUSSIA_min', 'USAGE_OUT_INT_VOICE_RUSSIA_max', 'USAGE_OUT_INT_VOICE_RUSSIA_last_nonnull_count', 'USAGE_OUT_INT_VOICE_RUSSIA_trend', 'USAGE_IN_ONNET_VOICE_first', 'USAGE_IN_ONNET_VOICE_last', 'USAGE_IN_ONNET_VOICE_mean', 'USAGE_IN_ONNET_VOICE_std', 'USAGE_IN_ONNET_VOICE_min', 'USAGE_IN_ONNET_VOICE_max', 'USAGE_IN_ONNET_VOICE_last_nonnull_count', 'USAGE_IN_ONNET_VOICE_trend', 'USAGE_IN_OFFNET_VOICE_first', 'USAGE_IN_OFFNET_VOICE_last', 'USAGE_IN_OFFNET_VOICE_mean', 'USAGE_IN_OFFNET_VOICE_std', 'USAGE_IN_OFFNET_VOICE_min', 'USAGE_IN_OFFNET_VOICE_max', 'USAGE_IN_OFFNET_VOICE_last_nonnull_count', 'USAGE_IN_OFFNET_VOICE_trend', 'USAGE_VALUELESS_INTERNET_first', 'USAGE_VALUELESS_INTERNET_last', 'USAGE_VALUELESS_INTERNET_mean', 'USAGE_VALUELESS_INTERNET_std', 'USAGE_VALUELESS_INTERNET_min', 'USAGE_VALUELESS_INTERNET_max', 'USAGE_VALUELESS_INTERNET_last_nonnull_count', 'USAGE_VALUELESS_INTERNET_trend', 'USAGE_INTERNET_first', 'USAGE_INTERNET_last', 'USAGE_INTERNET_mean', 'USAGE_INTERNET_std', 'USAGE_INTERNET_min', 'USAGE_INTERNET_max', 'USAGE_INTERNET_last_nonnull_count', 'USAGE_INTERNET_trend', 'USAGE_INTERNET_2G_first', 'USAGE_INTERNET_2G_last', 'USAGE_INTERNET_2G_mean', 'USAGE_INTERNET_2G_std', 'USAGE_INTERNET_2G_min', 'USAGE_INTERNET_2G_max', 'USAGE_INTERNET_2G_last_nonnull_count', 'USAGE_INTERNET_2G_trend', 'USAGE_INTERNET_3G_first', 'USAGE_INTERNET_3G_last', 'USAGE_INTERNET_3G_mean', 'USAGE_INTERNET_3G_std', 'USAGE_INTERNET_3G_min', 'USAGE_INTERNET_3G_max', 'USAGE_INTERNET_3G_last_nonnull_count', 'USAGE_INTERNET_3G_trend', 'USAGE_INTERNET_LTE_first', 'USAGE_INTERNET_LTE_last', 'USAGE_INTERNET_LTE_mean', 'USAGE_INTERNET_LTE_std', 'USAGE_INTERNET_LTE_min', 'USAGE_INTERNET_LTE_max', 'USAGE_INTERNET_LTE_last_nonnull_count', 'USAGE_INTERNET_LTE_trend', 'USAGE_INTERNET_3G_FREE_first', 'USAGE_INTERNET_3G_FREE_last', 'USAGE_INTERNET_3G_FREE_mean', 'USAGE_INTERNET_3G_FREE_std', 'USAGE_INTERNET_3G_FREE_min', 'USAGE_INTERNET_3G_FREE_max', 'USAGE_INTERNET_3G_FREE_last_nonnull_count', 'USAGE_INTERNET_3G_FREE_trend', 'USAGE_INTERNET_LTE_FREE_first', 'USAGE_INTERNET_LTE_FREE_last', 'USAGE_INTERNET_LTE_FREE_mean', 'USAGE_INTERNET_LTE_FREE_std', 'USAGE_INTERNET_LTE_FREE_min', 'USAGE_INTERNET_LTE_FREE_max', 'USAGE_INTERNET_LTE_FREE_last_nonnull_count', 'USAGE_INTERNET_LTE_FREE_trend', 'USAGE_OUT_OFFNET_O_VOICE_first', 'USAGE_OUT_OFFNET_O_VOICE_last', 'USAGE_OUT_OFFNET_O_VOICE_mean', 'USAGE_OUT_OFFNET_O_VOICE_std', 'USAGE_OUT_OFFNET_O_VOICE_min', 'USAGE_OUT_OFFNET_O_VOICE_max', 'USAGE_OUT_OFFNET_O_VOICE_last_nonnull_count', 'USAGE_OUT_OFFNET_O_VOICE_trend', 'USAGE_OUT_OFFNET_MEGACOM_VOICE_first', 'USAGE_OUT_OFFNET_MEGACOM_VOICE_last', 'USAGE_OUT_OFFNET_MEGACOM_VOICE_mean', 'USAGE_OUT_OFFNET_MEGACOM_VOICE_std', 'USAGE_OUT_OFFNET_MEGACOM_VOICE_min', 'USAGE_OUT_OFFNET_MEGACOM_VOICE_max', 'USAGE_OUT_OFFNET_MEGACOM_VOICE_last_nonnull_count', 'USAGE_OUT_OFFNET_MEGACOM_VOICE_trend', 'USAGE_IN_OFFNET_O_VOICE_first', 'USAGE_IN_OFFNET_O_VOICE_last', 'USAGE_IN_OFFNET_O_VOICE_mean', 'USAGE_IN_OFFNET_O_VOICE_std', 'USAGE_IN_OFFNET_O_VOICE_min', 'USAGE_IN_OFFNET_O_VOICE_max', 'USAGE_IN_OFFNET_O_VOICE_last_nonnull_count', 'USAGE_IN_OFFNET_O_VOICE_trend', 'USAGE_IN_OFFNET_MEGACOM_VOICE_first', 'USAGE_IN_OFFNET_MEGACOM_VOICE_last', 'USAGE_IN_OFFNET_MEGACOM_VOICE_mean', 'USAGE_IN_OFFNET_MEGACOM_VOICE_std', 'USAGE_IN_OFFNET_MEGACOM_VOICE_min', 'USAGE_IN_OFFNET_MEGACOM_VOICE_max', 'USAGE_IN_OFFNET_MEGACOM_VOICE_last_nonnull_count', 'USAGE_IN_OFFNET_MEGACOM_VOICE_trend', 'COUNT_SMS_first', 'COUNT_SMS_last', 'COUNT_SMS_mean', 'COUNT_SMS_std', 'COUNT_SMS_min', 'COUNT_SMS_max', 'COUNT_SMS_last_nonnull_count', 'COUNT_SMS_trend', 'REVENUE_VOICE_first', 'REVENUE_VOICE_last', 'REVENUE_VOICE_mean', 'REVENUE_VOICE_std', 'REVENUE_VOICE_min', 'REVENUE_VOICE_max', 'REVENUE_VOICE_last_nonnull_count', 'REVENUE_VOICE_trend', 'REVENUE_VOICE_TO_SERVICE_first', 'REVENUE_VOICE_TO_SERVICE_last', 'REVENUE_VOICE_TO_SERVICE_mean', 'REVENUE_VOICE_TO_SERVICE_std', 'REVENUE_VOICE_TO_SERVICE_min', 'REVENUE_VOICE_TO_SERVICE_max', 'REVENUE_VOICE_TO_SERVICE_last_nonnull_count', 'REVENUE_VOICE_TO_SERVICE_trend', 'REVENUE_OUT_ONNET_VOICE_first', 'REVENUE_OUT_ONNET_VOICE_last', 'REVENUE_OUT_ONNET_VOICE_mean', 'REVENUE_OUT_ONNET_VOICE_std', 'REVENUE_OUT_ONNET_VOICE_min', 'REVENUE_OUT_ONNET_VOICE_max', 'REVENUE_OUT_ONNET_VOICE_last_nonnull_count', 'REVENUE_OUT_ONNET_VOICE_trend', 'REVENUE_OUT_OFFNET_VOICE_first', 'REVENUE_OUT_OFFNET_VOICE_last', 'REVENUE_OUT_OFFNET_VOICE_mean', 'REVENUE_OUT_OFFNET_VOICE_std', 'REVENUE_OUT_OFFNET_VOICE_min', 'REVENUE_OUT_OFFNET_VOICE_max', 'REVENUE_OUT_OFFNET_VOICE_last_nonnull_count', 'REVENUE_OUT_OFFNET_VOICE_trend', 'REVENUE_OUT_CITY_VOICE_first', 'REVENUE_OUT_CITY_VOICE_last', 'REVENUE_OUT_CITY_VOICE_mean', 'REVENUE_OUT_CITY_VOICE_std', 'REVENUE_OUT_CITY_VOICE_min', 'REVENUE_OUT_CITY_VOICE_max', 'REVENUE_OUT_CITY_VOICE_last_nonnull_count', 'REVENUE_OUT_CITY_VOICE_trend', 'REVENUE_OUT_INT_VOICE_first', 'REVENUE_OUT_INT_VOICE_last', 'REVENUE_OUT_INT_VOICE_mean', 'REVENUE_OUT_INT_VOICE_std', 'REVENUE_OUT_INT_VOICE_min', 'REVENUE_OUT_INT_VOICE_max', 'REVENUE_OUT_INT_VOICE_last_nonnull_count', 'REVENUE_OUT_INT_VOICE_trend', 'REVENUE_INTERNET_PAYG_first', 'REVENUE_INTERNET_PAYG_last', 'REVENUE_INTERNET_PAYG_mean', 'REVENUE_INTERNET_PAYG_std', 'REVENUE_INTERNET_PAYG_min', 'REVENUE_INTERNET_PAYG_max', 'REVENUE_INTERNET_PAYG_last_nonnull_count', 'REVENUE_INTERNET_PAYG_trend', 'USAGE_INTERNET_NIGHT_first', 'USAGE_INTERNET_NIGHT_last', 'USAGE_INTERNET_NIGHT_mean', 'USAGE_INTERNET_NIGHT_std', 'USAGE_INTERNET_NIGHT_min', 'USAGE_INTERNET_NIGHT_max', 'USAGE_INTERNET_NIGHT_last_nonnull_count', 'USAGE_INTERNET_NIGHT_trend', 'USAGE_NUM_INTERNET_PAK_first', 'USAGE_NUM_INTERNET_PAK_last', 'USAGE_NUM_INTERNET_PAK_mean', 'USAGE_NUM_INTERNET_PAK_std', 'USAGE_NUM_INTERNET_PAK_min', 'USAGE_NUM_INTERNET_PAK_max', 'USAGE_NUM_INTERNET_PAK_last_nonnull_count', 'USAGE_NUM_INTERNET_PAK_trend', 'REVENUE_INTERNET_PAK_first', 'REVENUE_INTERNET_PAK_last', 'REVENUE_INTERNET_PAK_mean', 'REVENUE_INTERNET_PAK_std', 'REVENUE_INTERNET_PAK_min', 'REVENUE_INTERNET_PAK_max', 'REVENUE_INTERNET_PAK_last_nonnull_count', 'REVENUE_INTERNET_PAK_trend', 'INTERCONNECT_MN_IN_first', 'INTERCONNECT_MN_IN_last', 'INTERCONNECT_MN_IN_mean', 'INTERCONNECT_MN_IN_std', 'INTERCONNECT_MN_IN_min', 'INTERCONNECT_MN_IN_max', 'INTERCONNECT_MN_IN_last_nonnull_count', 'INTERCONNECT_MN_IN_trend', 'INTERCONNECT_MN_OUT_first', 'INTERCONNECT_MN_OUT_last', 'INTERCONNECT_MN_OUT_mean', 'INTERCONNECT_MN_OUT_std', 'INTERCONNECT_MN_OUT_min', 'INTERCONNECT_MN_OUT_max', 'INTERCONNECT_MN_OUT_last_nonnull_count', 'INTERCONNECT_MN_OUT_trend', 'INTERCONNECT_LOC_IN_first', 'INTERCONNECT_LOC_IN_last', 'INTERCONNECT_LOC_IN_mean', 'INTERCONNECT_LOC_IN_std', 'INTERCONNECT_LOC_IN_min', 'INTERCONNECT_LOC_IN_max', 'INTERCONNECT_LOC_IN_last_nonnull_count', 'INTERCONNECT_LOC_IN_trend', 'INTERCONNECT_LOC_OUT_first', 'INTERCONNECT_LOC_OUT_last', 'INTERCONNECT_LOC_OUT_mean', 'INTERCONNECT_LOC_OUT_std', 'INTERCONNECT_LOC_OUT_min', 'INTERCONNECT_LOC_OUT_max', 'INTERCONNECT_LOC_OUT_last_nonnull_count', 'INTERCONNECT_LOC_OUT_trend', 'REVENUE_TOTAL_INTERCONNECT_first', 'REVENUE_TOTAL_INTERCONNECT_last', 'REVENUE_TOTAL_INTERCONNECT_mean', 'REVENUE_TOTAL_INTERCONNECT_std', 'REVENUE_TOTAL_INTERCONNECT_min', 'REVENUE_TOTAL_INTERCONNECT_max', 'REVENUE_TOTAL_INTERCONNECT_last_nonnull_count', 'REVENUE_TOTAL_INTERCONNECT_trend', 'GM_first', 'GM_last', 'GM_mean', 'GM_std', 'GM_min', 'GM_max', 'GM_last_nonnull_count', 'GM_trend', 'REVENUE_TOTAL_first', 'REVENUE_TOTAL_last', 'REVENUE_TOTAL_mean', 'REVENUE_TOTAL_std', 'REVENUE_TOTAL_min', 'REVENUE_TOTAL_max', 'REVENUE_TOTAL_last_nonnull_count', 'REVENUE_TOTAL_trend', 'OTHER_CHARGES_first', 'OTHER_CHARGES_last', 'OTHER_CHARGES_mean', 'OTHER_CHARGES_std', 'OTHER_CHARGES_min', 'OTHER_CHARGES_max', 'OTHER_CHARGES_last_nonnull_count', 'OTHER_CHARGES_trend', 'USAGE_OUT_FREE_OFFNET_VOICE_first', 'USAGE_OUT_FREE_OFFNET_VOICE_last', 'USAGE_OUT_FREE_OFFNET_VOICE_mean', 'USAGE_OUT_FREE_OFFNET_VOICE_std', 'USAGE_OUT_FREE_OFFNET_VOICE_min', 'USAGE_OUT_FREE_OFFNET_VOICE_max', 'USAGE_OUT_FREE_OFFNET_VOICE_last_nonnull_count', 'USAGE_OUT_FREE_OFFNET_VOICE_trend', 'REVENUE_DAILY_ABONKA_first', 'REVENUE_DAILY_ABONKA_last', 'REVENUE_DAILY_ABONKA_mean', 'REVENUE_DAILY_ABONKA_std', 'REVENUE_DAILY_ABONKA_min', 'REVENUE_DAILY_ABONKA_max', 'REVENUE_DAILY_ABONKA_last_nonnull_count', 'REVENUE_DAILY_ABONKA_trend', 'USAGE_DAILY_ABONKA_first', 'USAGE_DAILY_ABONKA_last', 'USAGE_DAILY_ABONKA_mean', 'USAGE_DAILY_ABONKA_std', 'USAGE_DAILY_ABONKA_min', 'USAGE_DAILY_ABONKA_max', 'USAGE_DAILY_ABONKA_last_nonnull_count', 'USAGE_DAILY_ABONKA_trend', 'REVENUE_ROUMING_first', 'REVENUE_ROUMING_last', 'REVENUE_ROUMING_mean', 'REVENUE_ROUMING_std', 'REVENUE_ROUMING_min', 'REVENUE_ROUMING_max', 'REVENUE_ROUMING_last_nonnull_count', 'REVENUE_ROUMING_trend', 'USAGE_NUM_INC_first', 'USAGE_NUM_INC_last', 'USAGE_NUM_INC_mean', 'USAGE_NUM_INC_std', 'USAGE_NUM_INC_min', 'USAGE_NUM_INC_max', 'USAGE_NUM_INC_last_nonnull_count', 'USAGE_NUM_INC_trend', 'REVENUE_OFFNET_O_VOICE_first', 'REVENUE_OFFNET_O_VOICE_last', 'REVENUE_OFFNET_O_VOICE_mean', 'REVENUE_OFFNET_O_VOICE_std', 'REVENUE_OFFNET_O_VOICE_min', 'REVENUE_OFFNET_O_VOICE_max', 'REVENUE_OFFNET_O_VOICE_last_nonnull_count', 'REVENUE_OFFNET_O_VOICE_trend', 'REVENUE_OFFNET_MEGACOM_VOICE_first', 'REVENUE_OFFNET_MEGACOM_VOICE_last', 'REVENUE_OFFNET_MEGACOM_VOICE_mean', 'REVENUE_OFFNET_MEGACOM_VOICE_std', 'REVENUE_OFFNET_MEGACOM_VOICE_min', 'REVENUE_OFFNET_MEGACOM_VOICE_max', 'REVENUE_OFFNET_MEGACOM_VOICE_last_nonnull_count', 'REVENUE_OFFNET_MEGACOM_VOICE_trend', 'ROLY_VOICE_CHARGE_first', 'ROLY_VOICE_CHARGE_last', 'ROLY_VOICE_CHARGE_mean', 'ROLY_VOICE_CHARGE_std', 'ROLY_VOICE_CHARGE_min', 'ROLY_VOICE_CHARGE_max', 'ROLY_VOICE_CHARGE_last_nonnull_count', 'ROLY_VOICE_CHARGE_trend', 'ROLY_DATA_CHARGE_first', 'ROLY_DATA_CHARGE_last', 'ROLY_DATA_CHARGE_mean', 'ROLY_DATA_CHARGE_std', 'ROLY_DATA_CHARGE_min', 'ROLY_DATA_CHARGE_max', 'ROLY_DATA_CHARGE_last_nonnull_count', 'ROLY_DATA_CHARGE_trend', 'ROLY_GLOBAL_first', 'ROLY_GLOBAL_last', 'ROLY_GLOBAL_mean', 'ROLY_GLOBAL_std', 'ROLY_GLOBAL_min', 'ROLY_GLOBAL_max', 'ROLY_GLOBAL_last_nonnull_count', 'ROLY_GLOBAL_trend', 'TOTAL_MOU_first', 'TOTAL_MOU_last', 'TOTAL_MOU_mean', 'TOTAL_MOU_std', 'TOTAL_MOU_min', 'TOTAL_MOU_max', 'TOTAL_MOU_last_nonnull_count', 'TOTAL_MOU_trend', 'LIFETIME_TOTAL_first', 'LIFETIME_TOTAL_last', 'LIFETIME_TOTAL_mean', 'LIFETIME_TOTAL_std', 'LIFETIME_TOTAL_min', 'LIFETIME_TOTAL_max', 'LIFETIME_TOTAL_last_nonnull_count', 'LIFETIME_TOTAL_trend', 'days_from_dt_end_to_date_inactive_first', 'days_from_dt_end_to_date_inactive_last', 'days_from_dt_end_to_date_inactive_mean', 'days_from_dt_end_to_date_inactive_std', 'days_from_dt_end_to_date_inactive_min', 'days_from_dt_end_to_date_inactive_max', 'days_from_dt_end_to_date_inactive_last_nonnull_count', 'days_from_dt_end_to_date_inactive_trend', 'days_from_dt_end_to_date_lad_first', 'days_from_dt_end_to_date_lad_last', 'days_from_dt_end_to_date_lad_mean', 'days_from_dt_end_to_date_lad_std', 'days_from_dt_end_to_date_lad_min', 'days_from_dt_end_to_date_lad_max', 'days_from_dt_end_to_date_lad_last_nonnull_count', 'days_from_dt_end_to_date_lad_trend', 'days_from_dt_end_to_price_change_date_first', 'days_from_dt_end_to_price_change_date_last', 'days_from_dt_end_to_price_change_date_mean', 'days_from_dt_end_to_price_change_date_std', 'days_from_dt_end_to_price_change_date_min', 'days_from_dt_end_to_price_change_date_max', 'days_from_dt_end_to_price_change_date_last_nonnull_count', 'days_from_dt_end_to_price_change_date_trend', 'days_from_dt_end_to_act_date_first', 'days_from_dt_end_to_act_date_last', 'days_from_dt_end_to_act_date_mean', 'days_from_dt_end_to_act_date_std', 'days_from_dt_end_to_act_date_min', 'days_from_dt_end_to_act_date_max', 'days_from_dt_end_to_act_date_last_nonnull_count', 'days_from_dt_end_to_act_date_trend', 'days_from_dt_end_to_date_abonka_first', 'days_from_dt_end_to_date_abonka_last', 'days_from_dt_end_to_date_abonka_mean', 'days_from_dt_end_to_date_abonka_std', 'days_from_dt_end_to_date_abonka_min', 'days_from_dt_end_to_date_abonka_max', 'days_from_dt_end_to_date_abonka_last_nonnull_count', 'days_from_dt_end_to_date_abonka_trend', 'days_from_dt_end_to_date_contract_first', 'days_from_dt_end_to_date_contract_last', 'days_from_dt_end_to_date_contract_mean', 'days_from_dt_end_to_date_contract_std', 'days_from_dt_end_to_date_contract_min', 'days_from_dt_end_to_date_contract_max', 'days_from_dt_end_to_date_contract_last_nonnull_count', 'days_from_dt_end_to_date_contract_trend'] | /tmp/ipykernel_3308355/4151171959.py:2\n",
      "2025-10-30 15:56:55,360 | my_logger - INFO - Numeric wide columns: ['SUBSCRIPTION_FEE_w1', 'SUBSCRIPTION_FEE_w2', 'SUBSCRIPTION_FEE_w3', 'SUBSCRIPTION_FEE_w4', 'BALANCE_END_w1', 'BALANCE_END_w2', 'BALANCE_END_w3', 'BALANCE_END_w4', 'REVENUE_ABONKA_w1', 'REVENUE_ABONKA_w2', 'REVENUE_ABONKA_w3', 'REVENUE_ABONKA_w4', 'USAGE_ABONKA_TP_w1', 'USAGE_ABONKA_TP_w2', 'USAGE_ABONKA_TP_w3', 'USAGE_ABONKA_TP_w4', 'DAYS_WITHOUT_PAYMENT_w1', 'DAYS_WITHOUT_PAYMENT_w2', 'DAYS_WITHOUT_PAYMENT_w3', 'DAYS_WITHOUT_PAYMENT_w4', 'TOTAL_RECHARGE_w1', 'TOTAL_RECHARGE_w2', 'TOTAL_RECHARGE_w3', 'TOTAL_RECHARGE_w4', 'COUNT_RECHARGE_w1', 'COUNT_RECHARGE_w2', 'COUNT_RECHARGE_w3', 'COUNT_RECHARGE_w4', 'USAGE_NUM_OUT_w1', 'USAGE_NUM_OUT_w2', 'USAGE_NUM_OUT_w3', 'USAGE_NUM_OUT_w4', 'USAGE_OUT_ONNET_VOICE_w1', 'USAGE_OUT_ONNET_VOICE_w2', 'USAGE_OUT_ONNET_VOICE_w3', 'USAGE_OUT_ONNET_VOICE_w4', 'USAGE_OUT_OFFNET_VOICE_w1', 'USAGE_OUT_OFFNET_VOICE_w2', 'USAGE_OUT_OFFNET_VOICE_w3', 'USAGE_OUT_OFFNET_VOICE_w4', 'USAGE_OUT_CITY_VOICE_w1', 'USAGE_OUT_CITY_VOICE_w2', 'USAGE_OUT_CITY_VOICE_w3', 'USAGE_OUT_CITY_VOICE_w4', 'USAGE_OUT_INT_VOICE_w1', 'USAGE_OUT_INT_VOICE_w2', 'USAGE_OUT_INT_VOICE_w3', 'USAGE_OUT_INT_VOICE_w4', 'USAGE_OUT_INT_VOICE_RUSSIA_w1', 'USAGE_OUT_INT_VOICE_RUSSIA_w2', 'USAGE_OUT_INT_VOICE_RUSSIA_w3', 'USAGE_OUT_INT_VOICE_RUSSIA_w4', 'USAGE_IN_ONNET_VOICE_w1', 'USAGE_IN_ONNET_VOICE_w2', 'USAGE_IN_ONNET_VOICE_w3', 'USAGE_IN_ONNET_VOICE_w4', 'USAGE_IN_OFFNET_VOICE_w1', 'USAGE_IN_OFFNET_VOICE_w2', 'USAGE_IN_OFFNET_VOICE_w3', 'USAGE_IN_OFFNET_VOICE_w4', 'USAGE_VALUELESS_INTERNET_w1', 'USAGE_VALUELESS_INTERNET_w2', 'USAGE_VALUELESS_INTERNET_w3', 'USAGE_VALUELESS_INTERNET_w4', 'USAGE_INTERNET_w1', 'USAGE_INTERNET_w2', 'USAGE_INTERNET_w3', 'USAGE_INTERNET_w4', 'USAGE_INTERNET_2G_w1', 'USAGE_INTERNET_2G_w2', 'USAGE_INTERNET_2G_w3', 'USAGE_INTERNET_2G_w4', 'USAGE_INTERNET_3G_w1', 'USAGE_INTERNET_3G_w2', 'USAGE_INTERNET_3G_w3', 'USAGE_INTERNET_3G_w4', 'USAGE_INTERNET_LTE_w1', 'USAGE_INTERNET_LTE_w2', 'USAGE_INTERNET_LTE_w3', 'USAGE_INTERNET_LTE_w4', 'USAGE_INTERNET_3G_FREE_w1', 'USAGE_INTERNET_3G_FREE_w2', 'USAGE_INTERNET_3G_FREE_w3', 'USAGE_INTERNET_3G_FREE_w4', 'USAGE_INTERNET_LTE_FREE_w1', 'USAGE_INTERNET_LTE_FREE_w2', 'USAGE_INTERNET_LTE_FREE_w3', 'USAGE_INTERNET_LTE_FREE_w4', 'USAGE_OUT_OFFNET_O_VOICE_w1', 'USAGE_OUT_OFFNET_O_VOICE_w2', 'USAGE_OUT_OFFNET_O_VOICE_w3', 'USAGE_OUT_OFFNET_O_VOICE_w4', 'USAGE_OUT_OFFNET_MEGACOM_VOICE_w1', 'USAGE_OUT_OFFNET_MEGACOM_VOICE_w2', 'USAGE_OUT_OFFNET_MEGACOM_VOICE_w3', 'USAGE_OUT_OFFNET_MEGACOM_VOICE_w4', 'USAGE_IN_OFFNET_O_VOICE_w1', 'USAGE_IN_OFFNET_O_VOICE_w2', 'USAGE_IN_OFFNET_O_VOICE_w3', 'USAGE_IN_OFFNET_O_VOICE_w4', 'USAGE_IN_OFFNET_MEGACOM_VOICE_w1', 'USAGE_IN_OFFNET_MEGACOM_VOICE_w2', 'USAGE_IN_OFFNET_MEGACOM_VOICE_w3', 'USAGE_IN_OFFNET_MEGACOM_VOICE_w4', 'COUNT_SMS_w1', 'COUNT_SMS_w2', 'COUNT_SMS_w3', 'COUNT_SMS_w4', 'REVENUE_VOICE_w1', 'REVENUE_VOICE_w2', 'REVENUE_VOICE_w3', 'REVENUE_VOICE_w4', 'REVENUE_VOICE_TO_SERVICE_w1', 'REVENUE_VOICE_TO_SERVICE_w2', 'REVENUE_VOICE_TO_SERVICE_w3', 'REVENUE_VOICE_TO_SERVICE_w4', 'REVENUE_OUT_ONNET_VOICE_w1', 'REVENUE_OUT_ONNET_VOICE_w2', 'REVENUE_OUT_ONNET_VOICE_w3', 'REVENUE_OUT_ONNET_VOICE_w4', 'REVENUE_OUT_OFFNET_VOICE_w1', 'REVENUE_OUT_OFFNET_VOICE_w2', 'REVENUE_OUT_OFFNET_VOICE_w3', 'REVENUE_OUT_OFFNET_VOICE_w4', 'REVENUE_OUT_CITY_VOICE_w1', 'REVENUE_OUT_CITY_VOICE_w2', 'REVENUE_OUT_CITY_VOICE_w3', 'REVENUE_OUT_CITY_VOICE_w4', 'REVENUE_OUT_INT_VOICE_w1', 'REVENUE_OUT_INT_VOICE_w2', 'REVENUE_OUT_INT_VOICE_w3', 'REVENUE_OUT_INT_VOICE_w4', 'REVENUE_INTERNET_PAYG_w1', 'REVENUE_INTERNET_PAYG_w2', 'REVENUE_INTERNET_PAYG_w3', 'REVENUE_INTERNET_PAYG_w4', 'USAGE_INTERNET_NIGHT_w1', 'USAGE_INTERNET_NIGHT_w2', 'USAGE_INTERNET_NIGHT_w3', 'USAGE_INTERNET_NIGHT_w4', 'USAGE_NUM_INTERNET_PAK_w1', 'USAGE_NUM_INTERNET_PAK_w2', 'USAGE_NUM_INTERNET_PAK_w3', 'USAGE_NUM_INTERNET_PAK_w4', 'REVENUE_INTERNET_PAK_w1', 'REVENUE_INTERNET_PAK_w2', 'REVENUE_INTERNET_PAK_w3', 'REVENUE_INTERNET_PAK_w4', 'INTERCONNECT_MN_IN_w1', 'INTERCONNECT_MN_IN_w2', 'INTERCONNECT_MN_IN_w3', 'INTERCONNECT_MN_IN_w4', 'INTERCONNECT_MN_OUT_w1', 'INTERCONNECT_MN_OUT_w2', 'INTERCONNECT_MN_OUT_w3', 'INTERCONNECT_MN_OUT_w4', 'INTERCONNECT_LOC_IN_w1', 'INTERCONNECT_LOC_IN_w2', 'INTERCONNECT_LOC_IN_w3', 'INTERCONNECT_LOC_IN_w4', 'INTERCONNECT_LOC_OUT_w1', 'INTERCONNECT_LOC_OUT_w2', 'INTERCONNECT_LOC_OUT_w3', 'INTERCONNECT_LOC_OUT_w4', 'REVENUE_TOTAL_INTERCONNECT_w1', 'REVENUE_TOTAL_INTERCONNECT_w2', 'REVENUE_TOTAL_INTERCONNECT_w3', 'REVENUE_TOTAL_INTERCONNECT_w4', 'GM_w1', 'GM_w2', 'GM_w3', 'GM_w4', 'REVENUE_TOTAL_w1', 'REVENUE_TOTAL_w2', 'REVENUE_TOTAL_w3', 'REVENUE_TOTAL_w4', 'OTHER_CHARGES_w1', 'OTHER_CHARGES_w2', 'OTHER_CHARGES_w3', 'OTHER_CHARGES_w4', 'USAGE_OUT_FREE_OFFNET_VOICE_w1', 'USAGE_OUT_FREE_OFFNET_VOICE_w2', 'USAGE_OUT_FREE_OFFNET_VOICE_w3', 'USAGE_OUT_FREE_OFFNET_VOICE_w4', 'REVENUE_DAILY_ABONKA_w1', 'REVENUE_DAILY_ABONKA_w2', 'REVENUE_DAILY_ABONKA_w3', 'REVENUE_DAILY_ABONKA_w4', 'USAGE_DAILY_ABONKA_w1', 'USAGE_DAILY_ABONKA_w2', 'USAGE_DAILY_ABONKA_w3', 'USAGE_DAILY_ABONKA_w4', 'REVENUE_ROUMING_w1', 'REVENUE_ROUMING_w2', 'REVENUE_ROUMING_w3', 'REVENUE_ROUMING_w4', 'USAGE_NUM_INC_w1', 'USAGE_NUM_INC_w2', 'USAGE_NUM_INC_w3', 'USAGE_NUM_INC_w4', 'REVENUE_OFFNET_O_VOICE_w1', 'REVENUE_OFFNET_O_VOICE_w2', 'REVENUE_OFFNET_O_VOICE_w3', 'REVENUE_OFFNET_O_VOICE_w4', 'REVENUE_OFFNET_MEGACOM_VOICE_w1', 'REVENUE_OFFNET_MEGACOM_VOICE_w2', 'REVENUE_OFFNET_MEGACOM_VOICE_w3', 'REVENUE_OFFNET_MEGACOM_VOICE_w4', 'ROLY_VOICE_CHARGE_w1', 'ROLY_VOICE_CHARGE_w2', 'ROLY_VOICE_CHARGE_w3', 'ROLY_VOICE_CHARGE_w4', 'ROLY_DATA_CHARGE_w1', 'ROLY_DATA_CHARGE_w2', 'ROLY_DATA_CHARGE_w3', 'ROLY_DATA_CHARGE_w4', 'ROLY_GLOBAL_w1', 'ROLY_GLOBAL_w2', 'ROLY_GLOBAL_w3', 'ROLY_GLOBAL_w4', 'TOTAL_MOU_w1', 'TOTAL_MOU_w2', 'TOTAL_MOU_w3', 'TOTAL_MOU_w4', 'LIFETIME_TOTAL_w1', 'LIFETIME_TOTAL_w2', 'LIFETIME_TOTAL_w3', 'LIFETIME_TOTAL_w4', 'days_from_dt_end_to_date_inactive_w1', 'days_from_dt_end_to_date_inactive_w2', 'days_from_dt_end_to_date_inactive_w3', 'days_from_dt_end_to_date_inactive_w4', 'days_from_dt_end_to_date_lad_w1', 'days_from_dt_end_to_date_lad_w2', 'days_from_dt_end_to_date_lad_w3', 'days_from_dt_end_to_date_lad_w4', 'days_from_dt_end_to_price_change_date_w1', 'days_from_dt_end_to_price_change_date_w2', 'days_from_dt_end_to_price_change_date_w3', 'days_from_dt_end_to_price_change_date_w4', 'days_from_dt_end_to_act_date_w1', 'days_from_dt_end_to_act_date_w2', 'days_from_dt_end_to_act_date_w3', 'days_from_dt_end_to_act_date_w4', 'days_from_dt_end_to_date_abonka_w1', 'days_from_dt_end_to_date_abonka_w2', 'days_from_dt_end_to_date_abonka_w3', 'days_from_dt_end_to_date_abonka_w4', 'days_from_dt_end_to_date_contract_w1', 'days_from_dt_end_to_date_contract_w2', 'days_from_dt_end_to_date_contract_w3', 'days_from_dt_end_to_date_contract_w4'] | /tmp/ipykernel_3308355/4151171959.py:3\n",
      "2025-10-30 15:56:55,361 | my_logger - INFO - Numeric features table shape: (14120, 504) | /tmp/ipykernel_3308355/4151171959.py:4\n"
     ]
    }
   ],
   "source": [
    "num_tab, num_tab_columns, num_wide_columns = make_num_features_from_wide(num_wide, numeric_cols, COUNT_WEEKS)\n",
    "logger.info(f\"Created numeric features: {num_tab_columns}\")\n",
    "logger.info(f\"Numeric wide columns: {num_wide_columns}\")\n",
    "logger.info(f\"Numeric features table shape: {num_tab.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81e6b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 15:56:55,669 | my_logger - INFO - After joining numeric features, shape: (14120, 253) | /tmp/ipykernel_3308355/1463284494.py:2\n",
      "2025-10-30 15:56:55,983 | my_logger - INFO - After joining wide categorical features, shape: (14120, 369) | /tmp/ipykernel_3308355/1463284494.py:4\n",
      "2025-10-30 15:56:56,337 | my_logger - INFO - After joining numeric features, shape: (14120, 873) | /tmp/ipykernel_3308355/1463284494.py:6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 15:56:56,739 | my_logger - INFO - After joining categorical features, shape: (14120, 960) | /tmp/ipykernel_3308355/1463284494.py:8\n",
      "2025-10-30 15:56:57,098 | my_logger - INFO - After joining categorical _changes features, shape: (14120, 989) | /tmp/ipykernel_3308355/1463284494.py:10\n"
     ]
    }
   ],
   "source": [
    "df_table = tgt.join(num_wide[num_wide_columns], how='inner', on=index_columns)\n",
    "logger.info(f\"After joining numeric features, shape: {df_table.shape}\")\n",
    "df_table = df_table.join(cat_wide[cat_wide_columns], how='inner', on=index_columns)\n",
    "logger.info(f\"After joining wide categorical features, shape: {df_table.shape}\")\n",
    "df_table = df_table.join(num_tab[num_tab_columns], how='inner', on=index_columns)\n",
    "logger.info(f\"After joining numeric features, shape: {df_table.shape}\")\n",
    "df_table = df_table.join(cat_tab[cat_tab_columns], how='inner', on=index_columns)\n",
    "logger.info(f\"After joining categorical features, shape: {df_table.shape}\")\n",
    "df_table = df_table.join(cat_tab[num_feats], how='inner', on=index_columns)\n",
    "logger.info(f\"After joining categorical _changes features, shape: {df_table.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e8a5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 15:56:57,904 | my_logger - INFO - Features table saved to /data/aturov/scoring/data/processed/features_table_4_20_60_2025-10-30.parquet | /tmp/ipykernel_3308355/2801462665.py:2\n"
     ]
    }
   ],
   "source": [
    "df_table.to_parquet(config.environment.data_processed_path / f'{NAME_DATAFRAME_TABLE}_{CURRENT_DATE}.parquet', index=True)\n",
    "logger.info(f\"Features table saved to {config.environment.data_processed_path / f'{NAME_DATAFRAME_TABLE}_{CURRENT_DATE}.parquet'}\")\n",
    "# Сохранение категориальных признаков и числовых в json\n",
    "num_columns = num_wide_columns+num_tab_columns + num_feats\n",
    "cat_columns = cat_wide_columns+ cat_tab_columns\n",
    "with open(config.environment.data_processed_path / f'{NAME_DATAFRAME_TABLE}_num_columns_{CURRENT_DATE}.json', 'w') as f:\n",
    "    json.dump(num_columns, f)\n",
    "with open(config.environment.data_processed_path / f'{NAME_DATAFRAME_TABLE}_cat_columns_{CURRENT_DATE}.json', 'w') as f:\n",
    "    json.dump(cat_columns, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc14310e",
   "metadata": {},
   "source": [
    "### Добавим фичи из банковских данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a392044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_full_years(df_part):\n",
    "    # Робастный расчёт возраста (полные годы на дату открытия)\n",
    "    df_part['date_open'] = pd.to_datetime(df_part['date_open'], errors='coerce')\n",
    "    df_part['date_birth'] = pd.to_datetime(df_part['date_birth'], errors='coerce')\n",
    "\n",
    "    # опционально: сохраняем дни для отладки/аналитики\n",
    "    df_part['age_days'] = (df_part['date_open'] - df_part['date_birth']).dt.days\n",
    "\n",
    "    # вычисляем полные годы (векторно, без apply)\n",
    "    mask = df_part['date_open'].notna() & df_part['date_birth'].notna()\n",
    "    y_open = df_part.loc[mask, 'date_open'].dt.year\n",
    "    m_open = df_part.loc[mask, 'date_open'].dt.month\n",
    "    d_open = df_part.loc[mask, 'date_open'].dt.day\n",
    "    y_birth = df_part.loc[mask, 'date_birth'].dt.year\n",
    "    m_birth = df_part.loc[mask, 'date_birth'].dt.month\n",
    "    d_birth = df_part.loc[mask, 'date_birth'].dt.day\n",
    "\n",
    "    age_years = y_open - y_birth - ((m_open < m_birth) | ((m_open == m_birth) & (d_open < d_birth))).astype(int)\n",
    "    df_part.loc[mask, 'age'] = age_years\n",
    "\n",
    "    # nullable integer и фильтрация не реалистичных значений\n",
    "    df_part['age'] = df_part['age'].astype('Int64')\n",
    "    df_part.loc[~df_part['age'].between(0, 120), 'age'] = pd.NA\n",
    "    return df_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24e5691c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 15:56:58,429 | my_logger - INFO - Data shape from ClickHouse: (18631, 23) | /tmp/ipykernel_3308355/4205655774.py:21\n",
      "2025-10-30 15:56:58,450 | my_logger - INFO - Data shape from ClickHouse: (18631, 14) | /tmp/ipykernel_3308355/4205655774.py:30\n",
      "2025-10-30 15:56:58,450 | my_logger - INFO - Data shape from ClickHouse: (18631, 14) | /tmp/ipykernel_3308355/4205655774.py:32\n",
      "/tmp/ipykernel_3308355/4205655774.py:46: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_part[col].fillna(-1, inplace=True)\n",
      "/tmp/ipykernel_3308355/4205655774.py:49: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_part[col].fillna('unknown', inplace=True)\n",
      "/tmp/ipykernel_3308355/4205655774.py:49: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_part[col].fillna('unknown', inplace=True)\n",
      "/tmp/ipykernel_3308355/4205655774.py:49: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_part[col].fillna('unknown', inplace=True)\n",
      "2025-10-30 15:56:58,570 | my_logger - INFO - Data shape from ClickHouse after processing: (18410, 15) | /tmp/ipykernel_3308355/4205655774.py:53\n"
     ]
    }
   ],
   "source": [
    "def make_query(engine):\n",
    "    \"\"\"\n",
    "    Функция для выполнения SQL-запроса к базе данных и получения данных в виде DataFrame.\n",
    "    Параметры:\n",
    "    - engine: SQLAlchemy engine для подключения к базе данных.\n",
    "    Возвращает:\n",
    "    - DataFrame с результатами запроса.\n",
    "    \"\"\"\n",
    "\n",
    "    query = f\"\"\"\n",
    "            SELECT \n",
    "                *\n",
    "            FROM data_science.credits_subs_eldik_clean AS ce\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    data = pd.read_sql(query, engine)\n",
    "\n",
    "    return data\n",
    "df_part = make_query(clickhouse_engine)\n",
    "logger.info(f\"Data shape from ClickHouse: {df_part.shape}\")\n",
    "df_part.drop(columns=['phone_beeline', \n",
    "                      'inn_beeline', 'subs_eff_dt',  'interest_on_credit',\n",
    "                      'match_phone', 'match_inn', 'overdue_max',\n",
    "                        'total_overdue', 'status'], inplace=True)\n",
    "df_part.date_birth = pd.to_datetime(df_part.date_birth, errors='coerce')\n",
    "df_part.date_open = pd.to_datetime(df_part.date_open, errors='coerce')\n",
    "df_part.dropna(subset=['date_open', 'date_birth'], inplace=True)\n",
    "\n",
    "logger.info(f\"Data shape from ClickHouse: {df_part.shape}\")\n",
    "\n",
    "logger.info(f\"Data shape from ClickHouse: {df_part.shape}\")\n",
    "df_part = age_full_years(df_part)\n",
    "df_part.drop(columns=['date_birth'], inplace=True)\n",
    "\n",
    "cat_banking_features = ['sex',  'birthplace', 'marital_status', \n",
    "       #'name_region', 'city', 'street'\n",
    "       ]\n",
    "num_banking_features = ['age', 'age_days', 'prev_credit_count', 'sum_of_prev_credits',\n",
    "                        'contract_length', #'interest_on_credit', \n",
    "                        'summa'\n",
    "                        ]\n",
    "\n",
    "\n",
    "for col in num_banking_features:\n",
    "    df_part[col].fillna(-1, inplace=True)\n",
    "for col in cat_banking_features:\n",
    "    df_part[col] = df_part[col].astype(str)\n",
    "    df_part[col].fillna('unknown', inplace=True)\n",
    "\n",
    "\n",
    "df_part.drop_duplicates(subset=['id_request', 'inn_eldik','id_credit'], inplace=True)\n",
    "logger.info(f\"Data shape from ClickHouse after processing: {df_part.shape}\")\n",
    "df_part.set_index(index_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d648fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 15:56:59,063 | my_logger - INFO - Data shape after joining with features table: (13901, 998) | /tmp/ipykernel_3308355/2143672639.py:2\n"
     ]
    }
   ],
   "source": [
    "df_part = df_part.join(df_table, how='inner', on=index_columns)\n",
    "logger.info(f\"Data shape after joining with features table: {df_part.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af5082d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 15:56:59,864 | my_logger - INFO - Features saved to /data/aturov/scoring/data/processed/features_table_banking_4_20_60_2025-10-30.parquet | /tmp/ipykernel_3308355/2904767515.py:2\n"
     ]
    }
   ],
   "source": [
    "df_part.to_parquet(config.environment.data_processed_path / f'{NAME_DATAFRAME_BANKING}_{CURRENT_DATE}.parquet', index=True)\n",
    "logger.info(f\"Features saved to {config.environment.data_processed_path / f'{NAME_DATAFRAME_BANKING}_{CURRENT_DATE}.parquet'}\")\n",
    "cat_banking_features += cat_tab_columns + cat_wide_columns\n",
    "num_banking_features += num_tab_columns + num_wide_columns\n",
    "# Сохранение категориальных признаков и числовых в json\n",
    "import json\n",
    "num_columns = num_columns + num_banking_features\n",
    "cat_columns = cat_columns + cat_banking_features\n",
    "with open(config.environment.data_processed_path / f'{NAME_DATAFRAME_BANKING}_cat_columns_{CURRENT_DATE}.json', 'w') as f:\n",
    "    json.dump(cat_columns, f)\n",
    "with open(config.environment.data_processed_path / f'{NAME_DATAFRAME_BANKING}_num_columns_{CURRENT_DATE}.json', 'w') as f:\n",
    "    json.dump(num_columns, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
