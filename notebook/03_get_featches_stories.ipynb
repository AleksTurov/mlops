{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55f6153a",
   "metadata": {},
   "source": [
    "# Восстановленный ноутбук (как в логах/kernel dump)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d946d474",
   "metadata": {},
   "source": [
    "## Import библиотек и модулей\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49fb6a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 15:55:44,170 | my_logger - INFO - ✅ PostgreSQL engine создан | /data/aturov/scoring/src/database.py:21\n",
      "2025-10-30 15:55:44,209 | my_logger - INFO - ✅ ClickHouse engine создан | /data/aturov/scoring/src/database.py:36\n",
      "2025-10-30 15:55:44,210 | my_logger - INFO - ✅ IPDR ClickHouse engine создан | /data/aturov/scoring/src/database.py:46\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "# --- Настройка путей и sys.path ---\n",
    "# Добавляем корневую директорию проекта в sys.path для импорта кастомных модулей\n",
    "PROJECT_ROOT = Path().cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "from src.config import config\n",
    "from src.logger import logger\n",
    "from src.database import clickhouse_engine, postgres_engine, ipdr_engine   \n",
    "import pandas as pd, pyarrow\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "# --- Настройка путей и sys.path ---\n",
    "# Добавляем корневую директорию проекта в sys.path для импорта кастомных модулей\n",
    "PROJECT_ROOT = Path().cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "from src.config import config\n",
    "from src.logger import logger\n",
    "from src.database import clickhouse_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb968cd",
   "metadata": {},
   "source": [
    "## Constant variables and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81bfa28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "TARGET_COL = 'target'\n",
    "# Key params (tune as needed)\n",
    "COUNT_WEEKS = 5                # implies history length N = 12 COUNT_WEEKS = [5, 13]\n",
    "OVERDUE_DAYS_MAX = 20           # example threshold for 'bad' target OVERDUE_DAYS_MAX = [10, 30]\n",
    "TOTAL_OVERDUE = 60               # example threshold for total overdue days/amount TOTAL_OVERDUE = [60, 90]\n",
    "DATE_END = \"2025-04-30\"         # cap for date_open (filtering); set as needed\n",
    "SPLIT_DATE = \"2025-07-01\"       # train/val split by date_open\n",
    "\n",
    "NAME_DATAFRAME_WEEKS = f'features_weeks_{COUNT_WEEKS - 1}_{OVERDUE_DAYS_MAX}_{TOTAL_OVERDUE}' # имя файла с признаками по неделям for LSTMs models\n",
    "NAME_DATAFRAME_TABLE = f'features_table_{COUNT_WEEKS - 1}_{OVERDUE_DAYS_MAX}_{TOTAL_OVERDUE}' # имя файла с табличными признаками\n",
    "NAME_DATAFRAME_BANKING = f'features_table_banking_{COUNT_WEEKS - 1}_{OVERDUE_DAYS_MAX}_{TOTAL_OVERDUE}' # имя файла с табличными признаками банковскими признаками"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6826b161",
   "metadata": {},
   "source": [
    "## Dowlnload данных из ClickHouse и Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24a80749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_query(engine, number_weeks):\n",
    "    \"\"\"\n",
    "    Функция для выполнения SQL-запроса к базе данных и получения данных в виде DataFrame.\n",
    "    Параметры:\n",
    "    - engine: SQLAlchemy engine для подключения к базе данных.\n",
    "    Возвращает:\n",
    "    - DataFrame с результатами запроса.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "        SELECT\n",
    "            w.*,\n",
    "            ce.id_request,\n",
    "            ce.phone_eldik, \n",
    "            ce.inn_eldik, \n",
    "            ce.id_credit, \n",
    "            ce.date_open, \n",
    "            ce.subscription_id, \n",
    "            ce.match_phone, \n",
    "            ce.match_inn, \n",
    "            ce.overdue_max, \n",
    "            ce.total_overdue,\n",
    "            ce.status\n",
    "        FROM (\n",
    "            SELECT *,\n",
    "                toDate(DT) AS dt_day\n",
    "            FROM DWH.dm_datamart_weekly\n",
    "        ) AS w\n",
    "        GLOBAL INNER JOIN (\n",
    "            SELECT \n",
    "                ce.id_request,\n",
    "                ce.phone_eldik, \n",
    "                ce.inn_eldik, \n",
    "                ce.id_credit, \n",
    "                ce.date_open, \n",
    "                toUInt64(ce.subscription_id) AS subscription_id,\n",
    "                ce.match_phone, \n",
    "                ce.match_inn, \n",
    "                IFNULL(ce.overdue_max, 0) AS overdue_max,\n",
    "                IFNULL(ce.total_overdue, 0) AS total_overdue,\n",
    "                ce.status\n",
    "            FROM data_science.credits_subs_eldik_clean AS ce\n",
    "        ) AS ce\n",
    "        ON w.SUBS_ID = ce.subscription_id  and w.DT = toStartOfWeek(ce.date_open - INTERVAL {number_weeks} WEEK - INTERVAL 1 DAY, 1) \n",
    "        -- DT - дата начала интервала в понедельник, в понедельник данные загружаются за предыдущую неделю и не всегда успевают попасть в витрину\n",
    "        WHERE 1=1\n",
    "        --and w.SUBS_ID =  1993788\n",
    "        \n",
    "    \"\"\"    \n",
    "    data = pd.read_sql(query, engine)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7f9eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 15:55:44,724 | my_logger - INFO - Raw features shape: (221310, 110) | /tmp/ipykernel_3308355/4224803125.py:21\n",
      "2025-10-30 15:55:44,804 | my_logger - INFO - Initial features shape: (74468, 110) | /tmp/ipykernel_3308355/4224803125.py:23\n",
      "2025-10-30 15:55:44,869 | my_logger - INFO - Filtered by date_open <= 2025-04-30, shape: (56545, 110) | /tmp/ipykernel_3308355/4224803125.py:39\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(56545, 100)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_features_parts = []\n",
    "#for number_week in range(1, COUNT_WEEKS):  # от 1 до 12 недель включительно\n",
    "#    print(f\"COUNT_WEEKS = {number_week}\")\n",
    "#    df_part = make_query(clickhouse_engine, number_weeks=number_week)\n",
    "#    if df_part is None or df_part.empty:\n",
    "#        logger.warning(f\"COUNT_WEEKS = {number_week}, пустой датафрейм, пропускаем\")\n",
    "#        continue\n",
    "#    df_part['count_weeks'] = number_week\n",
    "#    logger.info(f\"COUNT_WEEKS = {number_week}, shape = {df_part.shape}\")\n",
    "#    df_features_parts.append(df_part)\n",
    "#\n",
    "#df_features = pd.concat(df_features_parts, ignore_index=True) if df_features_parts else pd.DataFrame()\n",
    "#\n",
    "## приведение типов и сортировка\n",
    "#if 'DT' in df_features.columns:\n",
    "#    df_features['DT'] = pd.to_datetime(df_features['DT'], errors='coerce')\n",
    "#df_features.sort_values(['subscription_id', 'id_credit', 'DT'], ascending=[True, True, False], inplace=True)\n",
    "#df_features = df_features.query('count_weeks != 0')  # удаляем нулевую неделю (пересечение с датой открытия кредита)\n",
    "#df_features.to_parquet(config.environment.data_raw_path / f'aturov_features_{CURRENT_DATE}.parquet', index=False)\n",
    "df_features = pd.read_parquet(config.environment.data_raw_path / 'aturov_features_2025-10-29.parquet')\n",
    "logger.info(f\"Raw features shape: {df_features.shape}\")\n",
    "df_features = df_features.query(f'count_weeks <= {COUNT_WEEKS - 1}') # оставляем только нужное количество недель\n",
    "logger.info(f\"Initial features shape: {df_features.shape}\")\n",
    "\n",
    "# formating\n",
    "df_features['TRANZ_FLAG'] = df_features['TRANZ_FLAG'].astype('boolean')\n",
    "df_features['FLAG_4G'] = df_features['FLAG_4G'].astype('boolean')\n",
    "df_features['ACTIVE_IND'] = df_features['ACTIVE_IND'].astype(str)\n",
    "df_features['FLAG_ABONKA'] = df_features['FLAG_ABONKA'].astype('boolean')\n",
    "df_features['MY_BEELINE_USER'] = df_features['MY_BEELINE_USER'].astype('boolean')\n",
    "df_features['BALANCE_USER'] = df_features['BALANCE_USER'].astype('boolean')\n",
    "df_features['MULTIPLAY'] = df_features['MULTIPLAY'].astype('boolean')\n",
    "df_features['M2M_FLAG'] = df_features['M2M_FLAG'].astype('boolean')\n",
    "df_features['match_inn'] = df_features['match_inn'].astype('boolean')\n",
    "df_features['match_phone'] = df_features['match_phone'].astype('boolean')\n",
    "# преобразование дат и создание новых признаков на основе разницы дат\n",
    "df_features['date_open'] = pd.to_datetime(df_features['date_open'], errors='coerce')\n",
    "df_features.dropna(subset=['date_open'], inplace=True)  # удаляем строки с некорректной датой открытия\n",
    "df_features = df_features[df_features['date_open'] <= pd.to_datetime(DATE_END)]  # фильтрация по дате открытия\n",
    "logger.info(f\"Filtered by date_open <= {DATE_END}, shape: {df_features.shape}\")\n",
    "df_features['DATE_INACTIVE'] = pd.to_datetime(df_features['DATE_INACTIVE'], errors='coerce')\n",
    "df_features['DATE_LAD'] = pd.to_datetime(df_features['DATE_LAD'], errors='coerce')\n",
    "df_features['PRICE_CHANGE_DATE'] = pd.to_datetime(df_features['PRICE_CHANGE_DATE'], errors='coerce')\n",
    "df_features['ACT_DATE'] = pd.to_datetime(df_features['ACT_DATE'], errors='coerce')\n",
    "df_features['DATE_ABONKA'] = pd.to_datetime(df_features['DATE_ABONKA'], errors='coerce')\n",
    "df_features['DATE_CONTRACT'] = pd.to_datetime(df_features['DATE_CONTRACT'], errors='coerce')\n",
    "df_features['DT'] = pd.to_datetime(df_features['DT'], errors='coerce')\n",
    "df_features['DT_END'] = (df_features['DT'] + pd.to_timedelta(7, unit='d')).dt.date # дата начала в понедельник влючительно и дата окончания в понедельник не включительно\n",
    "df_features['DT_END'] = pd.to_datetime(df_features['DT_END'], errors='coerce')\n",
    "\n",
    "# сразу посчитаем разницу дат \n",
    "df_features['days_from_dt_end_to_date_inactive'] = (df_features['DT_END'] - df_features['DATE_INACTIVE']).dt.days\n",
    "df_features['days_from_dt_end_to_date_inactive'].fillna(-999, inplace=True)  # заполняем пропуски большим числом\n",
    "df_features['days_from_dt_end_to_date_lad'] = (df_features['DT_END'] - df_features['DATE_LAD']).dt.days\n",
    "df_features['days_from_dt_end_to_date_lad'].fillna(-999, inplace=True)  # заполняем пропуски большим числом\n",
    "df_features['days_from_dt_end_to_price_change_date'] = (df_features['DT_END'] - df_features['PRICE_CHANGE_DATE']).dt.days\n",
    "df_features['days_from_dt_end_to_price_change_date'].fillna(-999, inplace=True)  # заполняем пропуски большим числом\n",
    "df_features['days_from_dt_end_to_act_date'] = (df_features['DT_END'] - df_features['ACT_DATE']).dt.days\n",
    "df_features['days_from_dt_end_to_act_date'].fillna(-999, inplace=True)  # заполняем пропуски большим числом\n",
    "df_features['days_from_dt_end_to_date_abonka'] = (df_features['DT_END'] - df_features['DATE_ABONKA']).dt.days\n",
    "df_features['days_from_dt_end_to_date_abonka'].fillna(-999, inplace=True)  # заполняем пропуски большим числом\n",
    "df_features['days_from_dt_end_to_date_contract'] = (df_features['DT_END'] - df_features['DATE_CONTRACT']).dt.days\n",
    "df_features['days_from_dt_end_to_date_contract'].fillna(-999, inplace=True)  # заполняем пропуски большим числом\n",
    "df_features.drop(columns=['DATE_INACTIVE', 'DATE_LAD', 'PRICE_CHANGE_DATE', 'ACT_DATE', 'DATE_ABONKA', 'DATE_CONTRACT', 'DT', 'DT_END'], inplace=True)\n",
    "# удалим те у кого много параметров\n",
    "#df_features.drop(columns=['CELL_MAX', 'CELL_ID', 'TAC', 'DEV_NAME'], inplace=True)\n",
    "\n",
    "# Создаем таргет: плохой клиент = 1, хороший = 0\n",
    "df_features['target'] = ((df_features['overdue_max'] >= OVERDUE_DAYS_MAX) | \n",
    "                        (df_features['total_overdue'] >= TOTAL_OVERDUE)| \n",
    "                        (df_features['status'].isin(['Отказано']))).astype(int)\n",
    "\n",
    "df_features.drop(columns=['IMEI', 'CTN','SUBS_ID', 'dt_day', 'FIRST_SIM', 'overdue_max', 'total_overdue', 'status'\n",
    "                          ,'USAGE_NUM_OFFNET_PAK', 'REVENUE_OFFNET_PAK'  # удалены из-за одного значения\n",
    "                          ], inplace=True)\n",
    "index_columns = ['id_request', 'subscription_id', 'phone_eldik', \n",
    "                 'inn_eldik', 'id_credit', 'date_open',\n",
    "                 ]\n",
    "sequential_columns = ['count_weeks']\n",
    "#df_features.set_index(index_columns, inplace=True)\n",
    "df_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a4c97d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CUST_LEVEL', 'STATUS', 'REGION_CELL', 'PRICE_PLAN', 'PRICE_PLAN_RU',\n",
       "       'PERIODICITY', 'SUBSCRIPTION_FEE', 'PREV_PRICE_PLAN', 'ORIG_PRICE_PLAN',\n",
       "       'BALANCE_END', 'REVENUE_ABONKA', 'USAGE_ABONKA_TP', 'TRANZ_FLAG',\n",
       "       'DAYS_WITHOUT_PAYMENT', 'TOTAL_RECHARGE', 'COUNT_RECHARGE', 'FLAG_4G',\n",
       "       'USAGE_NUM_OUT', 'USAGE_OUT_ONNET_VOICE', 'USAGE_OUT_OFFNET_VOICE',\n",
       "       'USAGE_OUT_CITY_VOICE', 'USAGE_OUT_INT_VOICE',\n",
       "       'USAGE_OUT_INT_VOICE_RUSSIA', 'USAGE_IN_ONNET_VOICE',\n",
       "       'USAGE_IN_OFFNET_VOICE', 'USAGE_VALUELESS_INTERNET', 'USAGE_INTERNET',\n",
       "       'USAGE_INTERNET_2G', 'USAGE_INTERNET_3G', 'USAGE_INTERNET_LTE',\n",
       "       'USAGE_INTERNET_3G_FREE', 'USAGE_INTERNET_LTE_FREE',\n",
       "       'USAGE_OUT_OFFNET_O_VOICE', 'USAGE_OUT_OFFNET_MEGACOM_VOICE',\n",
       "       'USAGE_IN_OFFNET_O_VOICE', 'USAGE_IN_OFFNET_MEGACOM_VOICE', 'COUNT_SMS',\n",
       "       'REVENUE_VOICE', 'REVENUE_VOICE_TO_SERVICE', 'REVENUE_OUT_ONNET_VOICE',\n",
       "       'REVENUE_OUT_OFFNET_VOICE', 'REVENUE_OUT_CITY_VOICE',\n",
       "       'REVENUE_OUT_INT_VOICE', 'REVENUE_INTERNET_PAYG',\n",
       "       'USAGE_INTERNET_NIGHT', 'USAGE_NUM_INTERNET_PAK',\n",
       "       'REVENUE_INTERNET_PAK', 'INTERCONNECT_MN_IN', 'INTERCONNECT_MN_OUT',\n",
       "       'INTERCONNECT_LOC_IN', 'INTERCONNECT_LOC_OUT',\n",
       "       'REVENUE_TOTAL_INTERCONNECT', 'GM', 'IVR_LANG', 'TAC', 'CELL_ID',\n",
       "       'DEV_NAME', 'DEV_TYPE', 'FLAG_DEVICE_4G', 'OS_NAME', 'REVENUE_TOTAL',\n",
       "       'OTHER_CHARGES', 'ACTIVE_IND', 'USAGE_OUT_FREE_OFFNET_VOICE',\n",
       "       'REVENUE_DAILY_ABONKA', 'USAGE_DAILY_ABONKA', 'REGION',\n",
       "       'REVENUE_ROUMING', 'USAGE_NUM_INC', 'REVENUE_OFFNET_O_VOICE',\n",
       "       'REVENUE_OFFNET_MEGACOM_VOICE', 'CELL_MAX', 'ROLY_VOICE_CHARGE',\n",
       "       'ROLY_DATA_CHARGE', 'ROLY_GLOBAL', 'FLAG_ABONKA', 'TOTAL_MOU',\n",
       "       'MY_BEELINE_USER', 'BALANCE_USER', 'MULTIPLAY', 'LIFETIME_TOTAL',\n",
       "       'M2M_FLAG', 'GENDER', 'AGE', 'id_request', 'phone_eldik', 'inn_eldik',\n",
       "       'id_credit', 'date_open', 'subscription_id', 'match_phone', 'match_inn',\n",
       "       'count_weeks', 'days_from_dt_end_to_date_inactive',\n",
       "       'days_from_dt_end_to_date_lad', 'days_from_dt_end_to_price_change_date',\n",
       "       'days_from_dt_end_to_act_date', 'days_from_dt_end_to_date_abonka',\n",
       "       'days_from_dt_end_to_date_contract', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca694b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dt</td>\n",
       "      <td>Дата начала расчётного периода</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ctn</td>\n",
       "      <td>CTN (номер) абонента</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subs_id</td>\n",
       "      <td>Ключ абонента</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cust_level</td>\n",
       "      <td>Тип клиента (B2B, B2C, Employee, etc)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>status</td>\n",
       "      <td>Статус абонента (активный, блокированный, прио...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>multiplay</td>\n",
       "      <td>Входит в мультиплеи (юзер одного из приложений...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>lifetime_total</td>\n",
       "      <td>Срок жизни абонента в нашей сети (дни)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>m2m_flag</td>\n",
       "      <td>Флаг M2M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>gender</td>\n",
       "      <td>Пол абонента по модели дата сайнтистов</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>age</td>\n",
       "      <td>Возраст абонента по модели дата сайнтистов</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          features                                        description\n",
       "0               dt                     Дата начала расчётного периода\n",
       "1              ctn                              CTN (номер) абонента \n",
       "2          subs_id                                      Ключ абонента\n",
       "3       cust_level              Тип клиента (B2B, B2C, Employee, etc)\n",
       "4           status  Статус абонента (активный, блокированный, прио...\n",
       "..             ...                                                ...\n",
       "93       multiplay  Входит в мультиплеи (юзер одного из приложений...\n",
       "94  lifetime_total             Срок жизни абонента в нашей сети (дни)\n",
       "95        m2m_flag                                           Флаг M2M\n",
       "96          gender             Пол абонента по модели дата сайнтистов\n",
       "97             age         Возраст абонента по модели дата сайнтистов\n",
       "\n",
       "[98 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe_features = pd.read_excel('/data/aturov/scoring/docs/Переменные_витрины_new_(3).xlsx')\n",
    "describe_features.columns = ['features', 'description']\n",
    "describe_features['features'] = describe_features['features'].str.lower()\n",
    "describe_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2683d300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: count, dtype: int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_stats = df_features.copy().query('count_weeks == 12')\n",
    "target_stats = target_stats['target'].value_counts().sort_index()\n",
    "target_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd39aa51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id_request',\n",
       " 'subscription_id',\n",
       " 'phone_eldik',\n",
       " 'inn_eldik',\n",
       " 'id_credit',\n",
       " 'date_open']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91f97de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14120, 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_12_weeks = df_features.groupby(index_columns)['count_weeks'].count().sort_values().reset_index()\n",
    "\n",
    "only_12_weeks = only_12_weeks[only_12_weeks['count_weeks'] == COUNT_WEEKS - 1]\n",
    "only_12_weeks.set_index(index_columns, inplace=True)\n",
    "only_12_weeks.drop(columns=['count_weeks'], inplace=True)\n",
    "only_12_weeks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74c8013a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56480, 94)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.set_index(index_columns, inplace=True)\n",
    "df_features = df_features.join(only_12_weeks, how='inner', on=index_columns)\n",
    "df_features.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a542d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4da56f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Dtypes & index columns ===\n",
    "datetime_cols = df_features.select_dtypes(include=['datetime64[ns]', 'datetimetz']).columns.tolist()\n",
    "categorical_cols = df_features.select_dtypes(include=['object','category','boolean']).columns.tolist()\n",
    "numeric_cols = df_features.select_dtypes(include=['float64','int64','float32','int32']).columns.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00713bb5",
   "metadata": {},
   "source": [
    "### Сохраним для дальенйшего обучения weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92a34191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 15:55:46,289 | my_logger - INFO - Features saved to /data/aturov/scoring/data/processed/features_weeks_4_20_60_2025-10-30.parquet | /tmp/ipykernel_3308355/1700928375.py:2\n"
     ]
    }
   ],
   "source": [
    "df_features.to_parquet(config.environment.data_processed_path / f'{NAME_DATAFRAME_WEEKS}_{CURRENT_DATE}.parquet', index=True)\n",
    "logger.info(f\"Features saved to {config.environment.data_processed_path / f'{NAME_DATAFRAME_WEEKS}_{CURRENT_DATE}.parquet'}\")\n",
    "# Сохранение категориальных признаков и числовых в json\n",
    "\n",
    "with open(config.environment.data_processed_path / f'{NAME_DATAFRAME_WEEKS}_num_columns_{CURRENT_DATE}.json', 'w') as f:\n",
    "    json.dump(numeric_cols, f)\n",
    "with open(config.environment.data_processed_path / f'{NAME_DATAFRAME_WEEKS}_cat_columns_{CURRENT_DATE}.json', 'w') as f:\n",
    "    json.dump(categorical_cols, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ebcbfb",
   "metadata": {},
   "source": [
    "### Переведем недели в фичи для обычных моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2966d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14120, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_cols = [col for col in numeric_cols if col not in (sequential_columns[0], TARGET_COL)]\n",
    "categorical_cols = [col for col in categorical_cols if col not in (sequential_columns[0], TARGET_COL)]\n",
    "\n",
    "tgt = df_features.reset_index(drop=False).drop_duplicates(subset=index_columns)[index_columns + [TARGET_COL]]\n",
    "tgt = tgt.set_index(index_columns)\n",
    "tgt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bdf9958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_weeks(df: pd.DataFrame, id_cols, seq_col, numeric_cols, categorical_cols, COUNT_WEEKS: int):\n",
    "    \"\"\"Возвращает num_wide и cat_wide (индекс = id_cols).\"\"\"\n",
    "    weeks = list(range(1, COUNT_WEEKS))\n",
    "    df_reset = df.reset_index(drop=False)\n",
    "\n",
    "    pnum = df_reset.pivot_table(index=id_cols, columns=seq_col, values=numeric_cols, aggfunc='first')\n",
    "    if isinstance(pnum.columns, pd.MultiIndex):\n",
    "        pnum.columns = [f\"{feat}_w{int(wk) if str(wk).replace('.0','').isdigit() else wk}\" for feat, wk in pnum.columns.to_list()]\n",
    "    else:\n",
    "        pnum.columns = [str(c) for c in pnum.columns.to_list()]\n",
    "    num_wide_cols = [f\"{feat}_w{wk}\" for feat in numeric_cols for wk in weeks if f\"{feat}_w{wk}\" in pnum.columns]\n",
    "    num_wide = pnum.reindex(columns=num_wide_cols)\n",
    "\n",
    "    pcat = df_reset.pivot_table(index=id_cols, columns=seq_col, values=categorical_cols, aggfunc='first')\n",
    "    if isinstance(pcat.columns, pd.MultiIndex):\n",
    "        pcat.columns = [f\"{feat}_w{int(wk) if str(wk).replace('.0','').isdigit() else wk}\" for feat, wk in pcat.columns.to_list()]\n",
    "    else:\n",
    "        pcat.columns = [str(c) for c in pcat.columns.to_list()]\n",
    "    cat_wide_cols = [f\"{feat}_w{wk}\" for feat in categorical_cols for wk in weeks if f\"{feat}_w{wk}\" in pcat.columns]\n",
    "    cat_wide = pcat.reindex(columns=cat_wide_cols)\n",
    "\n",
    "    return num_wide, cat_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc15816a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14120, 252), (14120, 116))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# получаем wide-таблицы\n",
    "num_wide, cat_wide = pivot_weeks(df_features.reset_index(drop=False), index_columns, sequential_columns[0],\n",
    "                                 numeric_cols, categorical_cols, COUNT_WEEKS)\n",
    "num_wide.shape, cat_wide.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f79f8f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cat_features_from_wide(cat_wide: pd.DataFrame, categorical_cols, COUNT_WEEKS: int, changes_as_bool: bool = False):\n",
    "    \"\"\"Для каждого categorical_cols возвращает first,last,mode,changes (DataFrame).\n",
    "    Параметры:\n",
    "      changes_as_bool — если True, колонка {feat}_changes будет булевой (True если было хотя бы одно изменение),\n",
    "                        иначе целочисленная (количество смен).\n",
    "    Возвращает: (cat_tab, cat_tab_columns, cat_wide_columns, num_feats)\n",
    "    \"\"\"\n",
    "    weeks = list(range(1, COUNT_WEEKS))\n",
    "    cat_feats = {}\n",
    "    num_feats = []\n",
    "    for feat in categorical_cols:\n",
    "        cols = [f\"{feat}_w{wk}\" for wk in weeks if f\"{feat}_w{wk}\" in cat_wide.columns]\n",
    "\n",
    "        if cols:\n",
    "            # берём значения по неделям, заменяем NA на 'missing' и приводим к str\n",
    "            C = cat_wide[cols].fillna(\"missing\").astype(str)\n",
    "            first_ser = C.get(f\"{feat}_w1\", C.iloc[:, 0]).astype(str)\n",
    "            last_ser = C.get(f\"{feat}_w{COUNT_WEEKS-1}\", C.iloc[:, -1]).astype(str)\n",
    "            # mode: безопасно получить первый модальный, иначе fallback на первый столбец\n",
    "            try:\n",
    "                mode_ser = C.mode(axis=1).iloc[:, 0].astype(str)\n",
    "            except Exception:\n",
    "                mode_ser = C.iloc[:, 0].astype(str)\n",
    "\n",
    "            # changes: подсчёт смен между соседними неделями\n",
    "            arr = C.to_numpy(dtype=object)\n",
    "            if arr.shape[1] > 1:\n",
    "                changes_cnt = (arr[:, 1:] != arr[:, :-1]).sum(axis=1).astype(np.int32)\n",
    "            else:\n",
    "                changes_cnt = np.zeros(C.shape[0], dtype=np.int32)\n",
    "\n",
    "            if changes_as_bool:\n",
    "                changes_ser = pd.Series(changes_cnt > 0, index=C.index, dtype=bool)\n",
    "            else:\n",
    "                changes_ser = pd.Series(changes_cnt, index=C.index, dtype=np.int32)\n",
    "\n",
    "            cat_feats[f\"{feat}_first\"] = first_ser\n",
    "            cat_feats[f\"{feat}_last\"] = last_ser\n",
    "            cat_feats[f\"{feat}_mode\"] = mode_ser\n",
    "            cat_feats[f\"{feat}_changes\"] = changes_ser\n",
    "            num_feats.append(f\"{feat}_changes\")\n",
    "        else:\n",
    "            # нет wide-столбцов для признака — ставим заглушки\n",
    "            idx = cat_wide.index\n",
    "            cat_feats[f\"{feat}_first\"] = pd.Series(\"missing\", index=idx, dtype=object)\n",
    "            cat_feats[f\"{feat}_last\"] = pd.Series(\"missing\", index=idx, dtype=object)\n",
    "            cat_feats[f\"{feat}_mode\"] = pd.Series(\"missing\", index=idx, dtype=object)\n",
    "            if changes_as_bool:\n",
    "                cat_feats[f\"{feat}_changes\"] = pd.Series(False, index=idx, dtype=bool)\n",
    "            else:\n",
    "                cat_feats[f\"{feat}_changes\"] = pd.Series(0, index=idx, dtype=np.int32)\n",
    "            num_feats.append(f\"{feat}_changes\")\n",
    "\n",
    "    cat_tab = pd.DataFrame(cat_feats, index=cat_wide.index)\n",
    "    cat_tab_columns = [c for c in cat_tab.columns.tolist() if c not in num_feats]\n",
    "    # Возвращаем: (таблица, список категориальных итоговых признаков, список колонок wide, список числовых _changes)\n",
    "    return cat_tab, cat_tab_columns, cat_wide.columns.tolist(), num_feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16acb5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 15:56:55,021 | my_logger - INFO - Created categorical features: ['CUST_LEVEL_first', 'CUST_LEVEL_last', 'CUST_LEVEL_mode', 'STATUS_first', 'STATUS_last', 'STATUS_mode', 'REGION_CELL_first', 'REGION_CELL_last', 'REGION_CELL_mode', 'PRICE_PLAN_first', 'PRICE_PLAN_last', 'PRICE_PLAN_mode', 'PRICE_PLAN_RU_first', 'PRICE_PLAN_RU_last', 'PRICE_PLAN_RU_mode', 'PERIODICITY_first', 'PERIODICITY_last', 'PERIODICITY_mode', 'PREV_PRICE_PLAN_first', 'PREV_PRICE_PLAN_last', 'PREV_PRICE_PLAN_mode', 'ORIG_PRICE_PLAN_first', 'ORIG_PRICE_PLAN_last', 'ORIG_PRICE_PLAN_mode', 'TRANZ_FLAG_first', 'TRANZ_FLAG_last', 'TRANZ_FLAG_mode', 'FLAG_4G_first', 'FLAG_4G_last', 'FLAG_4G_mode', 'IVR_LANG_first', 'IVR_LANG_last', 'IVR_LANG_mode', 'TAC_first', 'TAC_last', 'TAC_mode', 'CELL_ID_first', 'CELL_ID_last', 'CELL_ID_mode', 'DEV_NAME_first', 'DEV_NAME_last', 'DEV_NAME_mode', 'DEV_TYPE_first', 'DEV_TYPE_last', 'DEV_TYPE_mode', 'FLAG_DEVICE_4G_first', 'FLAG_DEVICE_4G_last', 'FLAG_DEVICE_4G_mode', 'OS_NAME_first', 'OS_NAME_last', 'OS_NAME_mode', 'ACTIVE_IND_first', 'ACTIVE_IND_last', 'ACTIVE_IND_mode', 'REGION_first', 'REGION_last', 'REGION_mode', 'CELL_MAX_first', 'CELL_MAX_last', 'CELL_MAX_mode', 'FLAG_ABONKA_first', 'FLAG_ABONKA_last', 'FLAG_ABONKA_mode', 'MY_BEELINE_USER_first', 'MY_BEELINE_USER_last', 'MY_BEELINE_USER_mode', 'BALANCE_USER_first', 'BALANCE_USER_last', 'BALANCE_USER_mode', 'MULTIPLAY_first', 'MULTIPLAY_last', 'MULTIPLAY_mode', 'M2M_FLAG_first', 'M2M_FLAG_last', 'M2M_FLAG_mode', 'GENDER_first', 'GENDER_last', 'GENDER_mode', 'AGE_first', 'AGE_last', 'AGE_mode', 'match_phone_first', 'match_phone_last', 'match_phone_mode', 'match_inn_first', 'match_inn_last', 'match_inn_mode'] | /tmp/ipykernel_3308355/683830168.py:2\n",
      "2025-10-30 15:56:55,023 | my_logger - INFO - Categorical wide columns: ['CUST_LEVEL_w1', 'CUST_LEVEL_w2', 'CUST_LEVEL_w3', 'CUST_LEVEL_w4', 'STATUS_w1', 'STATUS_w2', 'STATUS_w3', 'STATUS_w4', 'REGION_CELL_w1', 'REGION_CELL_w2', 'REGION_CELL_w3', 'REGION_CELL_w4', 'PRICE_PLAN_w1', 'PRICE_PLAN_w2', 'PRICE_PLAN_w3', 'PRICE_PLAN_w4', 'PRICE_PLAN_RU_w1', 'PRICE_PLAN_RU_w2', 'PRICE_PLAN_RU_w3', 'PRICE_PLAN_RU_w4', 'PERIODICITY_w1', 'PERIODICITY_w2', 'PERIODICITY_w3', 'PERIODICITY_w4', 'PREV_PRICE_PLAN_w1', 'PREV_PRICE_PLAN_w2', 'PREV_PRICE_PLAN_w3', 'PREV_PRICE_PLAN_w4', 'ORIG_PRICE_PLAN_w1', 'ORIG_PRICE_PLAN_w2', 'ORIG_PRICE_PLAN_w3', 'ORIG_PRICE_PLAN_w4', 'TRANZ_FLAG_w1', 'TRANZ_FLAG_w2', 'TRANZ_FLAG_w3', 'TRANZ_FLAG_w4', 'FLAG_4G_w1', 'FLAG_4G_w2', 'FLAG_4G_w3', 'FLAG_4G_w4', 'IVR_LANG_w1', 'IVR_LANG_w2', 'IVR_LANG_w3', 'IVR_LANG_w4', 'TAC_w1', 'TAC_w2', 'TAC_w3', 'TAC_w4', 'CELL_ID_w1', 'CELL_ID_w2', 'CELL_ID_w3', 'CELL_ID_w4', 'DEV_NAME_w1', 'DEV_NAME_w2', 'DEV_NAME_w3', 'DEV_NAME_w4', 'DEV_TYPE_w1', 'DEV_TYPE_w2', 'DEV_TYPE_w3', 'DEV_TYPE_w4', 'FLAG_DEVICE_4G_w1', 'FLAG_DEVICE_4G_w2', 'FLAG_DEVICE_4G_w3', 'FLAG_DEVICE_4G_w4', 'OS_NAME_w1', 'OS_NAME_w2', 'OS_NAME_w3', 'OS_NAME_w4', 'ACTIVE_IND_w1', 'ACTIVE_IND_w2', 'ACTIVE_IND_w3', 'ACTIVE_IND_w4', 'REGION_w1', 'REGION_w2', 'REGION_w3', 'REGION_w4', 'CELL_MAX_w1', 'CELL_MAX_w2', 'CELL_MAX_w3', 'CELL_MAX_w4', 'FLAG_ABONKA_w1', 'FLAG_ABONKA_w2', 'FLAG_ABONKA_w3', 'FLAG_ABONKA_w4', 'MY_BEELINE_USER_w1', 'MY_BEELINE_USER_w2', 'MY_BEELINE_USER_w3', 'MY_BEELINE_USER_w4', 'BALANCE_USER_w1', 'BALANCE_USER_w2', 'BALANCE_USER_w3', 'BALANCE_USER_w4', 'MULTIPLAY_w1', 'MULTIPLAY_w2', 'MULTIPLAY_w3', 'MULTIPLAY_w4', 'M2M_FLAG_w1', 'M2M_FLAG_w2', 'M2M_FLAG_w3', 'M2M_FLAG_w4', 'GENDER_w1', 'GENDER_w2', 'GENDER_w3', 'GENDER_w4', 'AGE_w1', 'AGE_w2', 'AGE_w3', 'AGE_w4', 'match_phone_w1', 'match_phone_w2', 'match_phone_w3', 'match_phone_w4', 'match_inn_w1', 'match_inn_w2', 'match_inn_w3', 'match_inn_w4'] | /tmp/ipykernel_3308355/683830168.py:3\n",
      "2025-10-30 15:56:55,023 | my_logger - INFO - Categorical features table shape: (14120, 116) | /tmp/ipykernel_3308355/683830168.py:4\n",
      "2025-10-30 15:56:55,024 | my_logger - INFO - Numerical features: ['CUST_LEVEL_changes', 'STATUS_changes', 'REGION_CELL_changes', 'PRICE_PLAN_changes', 'PRICE_PLAN_RU_changes', 'PERIODICITY_changes', 'PREV_PRICE_PLAN_changes', 'ORIG_PRICE_PLAN_changes', 'TRANZ_FLAG_changes', 'FLAG_4G_changes', 'IVR_LANG_changes', 'TAC_changes', 'CELL_ID_changes', 'DEV_NAME_changes', 'DEV_TYPE_changes', 'FLAG_DEVICE_4G_changes', 'OS_NAME_changes', 'ACTIVE_IND_changes', 'REGION_changes', 'CELL_MAX_changes', 'FLAG_ABONKA_changes', 'MY_BEELINE_USER_changes', 'BALANCE_USER_changes', 'MULTIPLAY_changes', 'M2M_FLAG_changes', 'GENDER_changes', 'AGE_changes', 'match_phone_changes', 'match_inn_changes'] | /tmp/ipykernel_3308355/683830168.py:5\n"
     ]
    }
   ],
   "source": [
    "cat_tab, cat_tab_columns, cat_wide_columns, num_feats = make_cat_features_from_wide(cat_wide, categorical_cols, COUNT_WEEKS)\n",
    "logger.info(f\"Created categorical features: {cat_tab_columns}\")\n",
    "logger.info(f\"Categorical wide columns: {cat_wide_columns}\")\n",
    "logger.info(f\"Categorical features table shape: {cat_tab.shape}\")\n",
    "logger.info(f\"Numerical features: {num_feats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08ca5d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_num_features_from_wide(num_wide: pd.DataFrame, numeric_cols, COUNT_WEEKS: int):\n",
    "    \"\"\"Для каждого numeric_cols возвращает first,last,mean,std,trend,min,max,last_nonnull_count.\"\"\"\n",
    "    num_aggs = {}\n",
    "    weeks = list(range(1, COUNT_WEEKS))\n",
    "    for feat in numeric_cols:\n",
    "        cols = [c for c in num_wide.columns if c.startswith(f\"{feat}_w\")]\n",
    "        if len(cols) == 0:\n",
    "            idx = num_wide.index\n",
    "            num_aggs[f\"{feat}_first\"] = pd.Series(np.nan, index=idx)\n",
    "            num_aggs[f\"{feat}_last\"]  = pd.Series(np.nan, index=idx)\n",
    "            num_aggs[f\"{feat}_mean\"]  = pd.Series(np.nan, index=idx)\n",
    "            num_aggs[f\"{feat}_std\"]   = pd.Series(np.nan, index=idx)\n",
    "            num_aggs[f\"{feat}_trend\"] = pd.Series(0.0, index=idx)\n",
    "            num_aggs[f\"{feat}_min\"]   = pd.Series(np.nan, index=idx)\n",
    "            num_aggs[f\"{feat}_max\"]   = pd.Series(np.nan, index=idx)\n",
    "            num_aggs[f\"{feat}_last_nonnull_count\"] = pd.Series(0, index=idx, dtype=int)\n",
    "            continue\n",
    "\n",
    "        mat = num_wide[cols].to_numpy(dtype=float)\n",
    "        mask = ~np.isnan(mat)\n",
    "        counts = mask.sum(axis=1)\n",
    "\n",
    "        # first = week1 (most distant) fallback -> first available\n",
    "        first_col = f\"{feat}_w1\"\n",
    "        last_col  = f\"{feat}_w{COUNT_WEEKS-1}\"\n",
    "        s_first = num_wide[first_col] if first_col in num_wide.columns else num_wide[cols].iloc[:, 0]\n",
    "        s_last  = num_wide[last_col]  if last_col  in num_wide.columns else num_wide[cols].iloc[:, -1]\n",
    "\n",
    "        num_aggs[f\"{feat}_first\"] = s_first.astype(float)\n",
    "        num_aggs[f\"{feat}_last\"]  = s_last.astype(float)\n",
    "\n",
    "        num_aggs[f\"{feat}_mean\"] = pd.Series(np.nanmean(mat, axis=1), index=num_wide.index)\n",
    "        num_aggs[f\"{feat}_std\"]  = pd.Series(np.nanstd(mat, axis=1), index=num_wide.index)\n",
    "        num_aggs[f\"{feat}_min\"]  = pd.Series(np.nanmin(mat, axis=1), index=num_wide.index)\n",
    "        num_aggs[f\"{feat}_max\"]  = pd.Series(np.nanmax(mat, axis=1), index=num_wide.index)\n",
    "        num_aggs[f\"{feat}_last_nonnull_count\"] = pd.Series(counts, index=num_wide.index)\n",
    "\n",
    "        # trend: векторно (линейный slope) по реальным номерам недель в cols\n",
    "        present_wks = np.array([int(c.rsplit('_w', 1)[1]) for c in cols], dtype=float)\n",
    "        mean_y = np.nanmean(mat, axis=1)\n",
    "        weighted_w = (mask * present_wks).sum(axis=1)\n",
    "        mean_w = np.where(counts > 0, weighted_w / np.maximum(counts, 1), 0.0)\n",
    "        num = ((present_wks - mean_w[:, None]) * (mat - mean_y[:, None])) * mask\n",
    "        numerator = np.nansum(num, axis=1)\n",
    "        denom = np.sum(((present_wks - mean_w[:, None]) ** 2) * mask, axis=1)\n",
    "        slope = np.where(denom > 0, numerator / denom, 0.0)\n",
    "        slope[counts < 2] = 0.0\n",
    "        num_aggs[f\"{feat}_trend\"] = pd.Series(slope, index=num_wide.index)\n",
    "\n",
    "    num_tab = pd.DataFrame(num_aggs, index=num_wide.index)\n",
    "    return num_tab, num_tab.columns.tolist(), num_wide.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd8d1156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3308355/1963821778.py:32: RuntimeWarning: Mean of empty slice\n",
      "  num_aggs[f\"{feat}_mean\"] = pd.Series(np.nanmean(mat, axis=1), index=num_wide.index)\n",
      "/data/aturov/scoring/.venv/lib/python3.10/site-packages/numpy/lib/_nanfunctions_impl.py:2019: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/tmp/ipykernel_3308355/1963821778.py:34: RuntimeWarning: All-NaN slice encountered\n",
      "  num_aggs[f\"{feat}_min\"]  = pd.Series(np.nanmin(mat, axis=1), index=num_wide.index)\n",
      "/tmp/ipykernel_3308355/1963821778.py:35: RuntimeWarning: All-NaN slice encountered\n",
      "  num_aggs[f\"{feat}_max\"]  = pd.Series(np.nanmax(mat, axis=1), index=num_wide.index)\n",
      "/tmp/ipykernel_3308355/1963821778.py:40: RuntimeWarning: Mean of empty slice\n",
      "  mean_y = np.nanmean(mat, axis=1)\n",
      "/tmp/ipykernel_3308355/1963821778.py:46: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = np.where(denom > 0, numerator / denom, 0.0)\n",
      "/tmp/ipykernel_3308355/1963821778.py:32: RuntimeWarning: Mean of empty slice\n",
      "  num_aggs[f\"{feat}_mean\"] = pd.Series(np.nanmean(mat, axis=1), index=num_wide.index)\n",
      "/data/aturov/scoring/.venv/lib/python3.10/site-packages/numpy/lib/_nanfunctions_impl.py:2019: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/tmp/ipykernel_3308355/1963821778.py:34: RuntimeWarning: All-NaN slice encountered\n",
      "  num_aggs[f\"{feat}_min\"]  = pd.Series(np.nanmin(mat, axis=1), index=num_wide.index)\n",
      "/tmp/ipykernel_3308355/1963821778.py:35: RuntimeWarning: All-NaN slice encountered\n",
      "  num_aggs[f\"{feat}_max\"]  = pd.Series(np.nanmax(mat, axis=1), index=num_wide.index)\n",
      "/tmp/ipykernel_3308355/1963821778.py:40: RuntimeWarning: Mean of empty slice\n",
      "  mean_y = np.nanmean(mat, axis=1)\n",
      "/tmp/ipykernel_3308355/1963821778.py:46: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = np.where(denom > 0, numerator / denom, 0.0)\n",
      "2025-10-30 15:56:55,359 | my_logger - INFO - Created numeric features: ['SUBSCRIPTION_FEE_first', 'SUBSCRIPTION_FEE_last', 'SUBSCRIPTION_FEE_mean', 'SUBSCRIPTION_FEE_std', 'SUBSCRIPTION_FEE_min', 'SUBSCRIPTION_FEE_max', 'SUBSCRIPTION_FEE_last_nonnull_count', 'SUBSCRIPTION_FEE_trend', 'BALANCE_END_first', 'BALANCE_END_last', 'BALANCE_END_mean', 'BALANCE_END_std', 'BALANCE_END_min', 'BALANCE_END_max', 'BALANCE_END_last_nonnull_count', 'BALANCE_END_trend', 'REVENUE_ABONKA_first', 'REVENUE_ABONKA_last', 'REVENUE_ABONKA_mean', 'REVENUE_ABONKA_std', 'REVENUE_ABONKA_min', 'REVENUE_ABONKA_max', 'REVENUE_ABONKA_last_nonnull_count', 'REVENUE_ABONKA_trend', 'USAGE_ABONKA_TP_first', 'USAGE_ABONKA_TP_last', 'USAGE_ABONKA_TP_mean', 'USAGE_ABONKA_TP_std', 'USAGE_ABONKA_TP_min', 'USAGE_ABONKA_TP_max', 'USAGE_ABONKA_TP_last_nonnull_count', 'USAGE_ABONKA_TP_trend', 'DAYS_WITHOUT_PAYMENT_first', 'DAYS_WITHOUT_PAYMENT_last', 'DAYS_WITHOUT_PAYMENT_mean', 'DAYS_WITHOUT_PAYMENT_std', 'DAYS_WITHOUT_PAYMENT_min', 'DAYS_WITHOUT_PAYMENT_max', 'DAYS_WITHOUT_PAYMENT_last_nonnull_count', 'DAYS_WITHOUT_PAYMENT_trend', 'TOTAL_RECHARGE_first', 'TOTAL_RECHARGE_last', 'TOTAL_RECHARGE_mean', 'TOTAL_RECHARGE_std', 'TOTAL_RECHARGE_min', 'TOTAL_RECHARGE_max', 'TOTAL_RECHARGE_last_nonnull_count', 'TOTAL_RECHARGE_trend', 'COUNT_RECHARGE_first', 'COUNT_RECHARGE_last', 'COUNT_RECHARGE_mean', 'COUNT_RECHARGE_std', 'COUNT_RECHARGE_min', 'COUNT_RECHARGE_max', 'COUNT_RECHARGE_last_nonnull_count', 'COUNT_RECHARGE_trend', 'USAGE_NUM_OUT_first', 'USAGE_NUM_OUT_last', 'USAGE_NUM_OUT_mean', 'USAGE_NUM_OUT_std', 'USAGE_NUM_OUT_min', 'USAGE_NUM_OUT_max', 'USAGE_NUM_OUT_last_nonnull_count', 'USAGE_NUM_OUT_trend', 'USAGE_OUT_ONNET_VOICE_first', 'USAGE_OUT_ONNET_VOICE_last', 'USAGE_OUT_ONNET_VOICE_mean', 'USAGE_OUT_ONNET_VOICE_std', 'USAGE_OUT_ONNET_VOICE_min', 'USAGE_OUT_ONNET_VOICE_max', 'USAGE_OUT_ONNET_VOICE_last_nonnull_count', 'USAGE_OUT_ONNET_VOICE_trend', 'USAGE_OUT_OFFNET_VOICE_first', 'USAGE_OUT_OFFNET_VOICE_last', 'USAGE_OUT_OFFNET_VOICE_mean', 'USAGE_OUT_OFFNET_VOICE_std', 'USAGE_OUT_OFFNET_VOICE_min', 'USAGE_OUT_OFFNET_VOICE_max', 'USAGE_OUT_OFFNET_VOICE_last_nonnull_count', 'USAGE_OUT_OFFNET_VOICE_trend', 'USAGE_OUT_CITY_VOICE_first', 'USAGE_OUT_CITY_VOICE_last', 'USAGE_OUT_CITY_VOICE_mean', 'USAGE_OUT_CITY_VOICE_std', 'USAGE_OUT_CITY_VOICE_min', 'USAGE_OUT_CITY_VOICE_max', 'USAGE_OUT_CITY_VOICE_last_nonnull_count', 'USAGE_OUT_CITY_VOICE_trend', 'USAGE_OUT_INT_VOICE_first', 'USAGE_OUT_INT_VOICE_last', 'USAGE_OUT_INT_VOICE_mean', 'USAGE_OUT_INT_VOICE_std', 'USAGE_OUT_INT_VOICE_min', 'USAGE_OUT_INT_VOICE_max', 'USAGE_OUT_INT_VOICE_last_nonnull_count', 'USAGE_OUT_INT_VOICE_trend', 'USAGE_OUT_INT_VOICE_RUSSIA_first', 'USAGE_OUT_INT_VOICE_RUSSIA_last', 'USAGE_OUT_INT_VOICE_RUSSIA_mean', 'USAGE_OUT_INT_VOICE_RUSSIA_std', 'USAGE_OUT_INT_VOICE_RUSSIA_min', 'USAGE_OUT_INT_VOICE_RUSSIA_max', 'USAGE_OUT_INT_VOICE_RUSSIA_last_nonnull_count', 'USAGE_OUT_INT_VOICE_RUSSIA_trend', 'USAGE_IN_ONNET_VOICE_first', 'USAGE_IN_ONNET_VOICE_last', 'USAGE_IN_ONNET_VOICE_mean', 'USAGE_IN_ONNET_VOICE_std', 'USAGE_IN_ONNET_VOICE_min', 'USAGE_IN_ONNET_VOICE_max', 'USAGE_IN_ONNET_VOICE_last_nonnull_count', 'USAGE_IN_ONNET_VOICE_trend', 'USAGE_IN_OFFNET_VOICE_first', 'USAGE_IN_OFFNET_VOICE_last', 'USAGE_IN_OFFNET_VOICE_mean', 'USAGE_IN_OFFNET_VOICE_std', 'USAGE_IN_OFFNET_VOICE_min', 'USAGE_IN_OFFNET_VOICE_max', 'USAGE_IN_OFFNET_VOICE_last_nonnull_count', 'USAGE_IN_OFFNET_VOICE_trend', 'USAGE_VALUELESS_INTERNET_first', 'USAGE_VALUELESS_INTERNET_last', 'USAGE_VALUELESS_INTERNET_mean', 'USAGE_VALUELESS_INTERNET_std', 'USAGE_VALUELESS_INTERNET_min', 'USAGE_VALUELESS_INTERNET_max', 'USAGE_VALUELESS_INTERNET_last_nonnull_count', 'USAGE_VALUELESS_INTERNET_trend', 'USAGE_INTERNET_first', 'USAGE_INTERNET_last', 'USAGE_INTERNET_mean', 'USAGE_INTERNET_std', 'USAGE_INTERNET_min', 'USAGE_INTERNET_max', 'USAGE_INTERNET_last_nonnull_count', 'USAGE_INTERNET_trend', 'USAGE_INTERNET_2G_first', 'USAGE_INTERNET_2G_last', 'USAGE_INTERNET_2G_mean', 'USAGE_INTERNET_2G_std', 'USAGE_INTERNET_2G_min', 'USAGE_INTERNET_2G_max', 'USAGE_INTERNET_2G_last_nonnull_count', 'USAGE_INTERNET_2G_trend', 'USAGE_INTERNET_3G_first', 'USAGE_INTERNET_3G_last', 'USAGE_INTERNET_3G_mean', 'USAGE_INTERNET_3G_std', 'USAGE_INTERNET_3G_min', 'USAGE_INTERNET_3G_max', 'USAGE_INTERNET_3G_last_nonnull_count', 'USAGE_INTERNET_3G_trend', 'USAGE_INTERNET_LTE_first', 'USAGE_INTERNET_LTE_last', 'USAGE_INTERNET_LTE_mean', 'USAGE_INTERNET_LTE_std', 'USAGE_INTERNET_LTE_min', 'USAGE_INTERNET_LTE_max', 'USAGE_INTERNET_LTE_last_nonnull_count', 'USAGE_INTERNET_LTE_trend', 'USAGE_INTERNET_3G_FREE_first', 'USAGE_INTERNET_3G_FREE_last', 'USAGE_INTERNET_3G_FREE_mean', 'USAGE_INTERNET_3G_FREE_std', 'USAGE_INTERNET_3G_FREE_min', 'USAGE_INTERNET_3G_FREE_max', 'USAGE_INTERNET_3G_FREE_last_nonnull_count', 'USAGE_INTERNET_3G_FREE_trend', 'USAGE_INTERNET_LTE_FREE_first', 'USAGE_INTERNET_LTE_FREE_last', 'USAGE_INTERNET_LTE_FREE_mean', 'USAGE_INTERNET_LTE_FREE_std', 'USAGE_INTERNET_LTE_FREE_min', 'USAGE_INTERNET_LTE_FREE_max', 'USAGE_INTERNET_LTE_FREE_last_nonnull_count', 'USAGE_INTERNET_LTE_FREE_trend', 'USAGE_OUT_OFFNET_O_VOICE_first', 'USAGE_OUT_OFFNET_O_VOICE_last', 'USAGE_OUT_OFFNET_O_VOICE_mean', 'USAGE_OUT_OFFNET_O_VOICE_std', 'USAGE_OUT_OFFNET_O_VOICE_min', 'USAGE_OUT_OFFNET_O_VOICE_max', 'USAGE_OUT_OFFNET_O_VOICE_last_nonnull_count', 'USAGE_OUT_OFFNET_O_VOICE_trend', 'USAGE_OUT_OFFNET_MEGACOM_VOICE_first', 'USAGE_OUT_OFFNET_MEGACOM_VOICE_last', 'USAGE_OUT_OFFNET_MEGACOM_VOICE_mean', 'USAGE_OUT_OFFNET_MEGACOM_VOICE_std', 'USAGE_OUT_OFFNET_MEGACOM_VOICE_min', 'USAGE_OUT_OFFNET_MEGACOM_VOICE_max', 'USAGE_OUT_OFFNET_MEGACOM_VOICE_last_nonnull_count', 'USAGE_OUT_OFFNET_MEGACOM_VOICE_trend', 'USAGE_IN_OFFNET_O_VOICE_first', 'USAGE_IN_OFFNET_O_VOICE_last', 'USAGE_IN_OFFNET_O_VOICE_mean', 'USAGE_IN_OFFNET_O_VOICE_std', 'USAGE_IN_OFFNET_O_VOICE_min', 'USAGE_IN_OFFNET_O_VOICE_max', 'USAGE_IN_OFFNET_O_VOICE_last_nonnull_count', 'USAGE_IN_OFFNET_O_VOICE_trend', 'USAGE_IN_OFFNET_MEGACOM_VOICE_first', 'USAGE_IN_OFFNET_MEGACOM_VOICE_last', 'USAGE_IN_OFFNET_MEGACOM_VOICE_mean', 'USAGE_IN_OFFNET_MEGACOM_VOICE_std', 'USAGE_IN_OFFNET_MEGACOM_VOICE_min', 'USAGE_IN_OFFNET_MEGACOM_VOICE_max', 'USAGE_IN_OFFNET_MEGACOM_VOICE_last_nonnull_count', 'USAGE_IN_OFFNET_MEGACOM_VOICE_trend', 'COUNT_SMS_first', 'COUNT_SMS_last', 'COUNT_SMS_mean', 'COUNT_SMS_std', 'COUNT_SMS_min', 'COUNT_SMS_max', 'COUNT_SMS_last_nonnull_count', 'COUNT_SMS_trend', 'REVENUE_VOICE_first', 'REVENUE_VOICE_last', 'REVENUE_VOICE_mean', 'REVENUE_VOICE_std', 'REVENUE_VOICE_min', 'REVENUE_VOICE_max', 'REVENUE_VOICE_last_nonnull_count', 'REVENUE_VOICE_trend', 'REVENUE_VOICE_TO_SERVICE_first', 'REVENUE_VOICE_TO_SERVICE_last', 'REVENUE_VOICE_TO_SERVICE_mean', 'REVENUE_VOICE_TO_SERVICE_std', 'REVENUE_VOICE_TO_SERVICE_min', 'REVENUE_VOICE_TO_SERVICE_max', 'REVENUE_VOICE_TO_SERVICE_last_nonnull_count', 'REVENUE_VOICE_TO_SERVICE_trend', 'REVENUE_OUT_ONNET_VOICE_first', 'REVENUE_OUT_ONNET_VOICE_last', 'REVENUE_OUT_ONNET_VOICE_mean', 'REVENUE_OUT_ONNET_VOICE_std', 'REVENUE_OUT_ONNET_VOICE_min', 'REVENUE_OUT_ONNET_VOICE_max', 'REVENUE_OUT_ONNET_VOICE_last_nonnull_count', 'REVENUE_OUT_ONNET_VOICE_trend', 'REVENUE_OUT_OFFNET_VOICE_first', 'REVENUE_OUT_OFFNET_VOICE_last', 'REVENUE_OUT_OFFNET_VOICE_mean', 'REVENUE_OUT_OFFNET_VOICE_std', 'REVENUE_OUT_OFFNET_VOICE_min', 'REVENUE_OUT_OFFNET_VOICE_max', 'REVENUE_OUT_OFFNET_VOICE_last_nonnull_count', 'REVENUE_OUT_OFFNET_VOICE_trend', 'REVENUE_OUT_CITY_VOICE_first', 'REVENUE_OUT_CITY_VOICE_last', 'REVENUE_OUT_CITY_VOICE_mean', 'REVENUE_OUT_CITY_VOICE_std', 'REVENUE_OUT_CITY_VOICE_min', 'REVENUE_OUT_CITY_VOICE_max', 'REVENUE_OUT_CITY_VOICE_last_nonnull_count', 'REVENUE_OUT_CITY_VOICE_trend', 'REVENUE_OUT_INT_VOICE_first', 'REVENUE_OUT_INT_VOICE_last', 'REVENUE_OUT_INT_VOICE_mean', 'REVENUE_OUT_INT_VOICE_std', 'REVENUE_OUT_INT_VOICE_min', 'REVENUE_OUT_INT_VOICE_max', 'REVENUE_OUT_INT_VOICE_last_nonnull_count', 'REVENUE_OUT_INT_VOICE_trend', 'REVENUE_INTERNET_PAYG_first', 'REVENUE_INTERNET_PAYG_last', 'REVENUE_INTERNET_PAYG_mean', 'REVENUE_INTERNET_PAYG_std', 'REVENUE_INTERNET_PAYG_min', 'REVENUE_INTERNET_PAYG_max', 'REVENUE_INTERNET_PAYG_last_nonnull_count', 'REVENUE_INTERNET_PAYG_trend', 'USAGE_INTERNET_NIGHT_first', 'USAGE_INTERNET_NIGHT_last', 'USAGE_INTERNET_NIGHT_mean', 'USAGE_INTERNET_NIGHT_std', 'USAGE_INTERNET_NIGHT_min', 'USAGE_INTERNET_NIGHT_max', 'USAGE_INTERNET_NIGHT_last_nonnull_count', 'USAGE_INTERNET_NIGHT_trend', 'USAGE_NUM_INTERNET_PAK_first', 'USAGE_NUM_INTERNET_PAK_last', 'USAGE_NUM_INTERNET_PAK_mean', 'USAGE_NUM_INTERNET_PAK_std', 'USAGE_NUM_INTERNET_PAK_min', 'USAGE_NUM_INTERNET_PAK_max', 'USAGE_NUM_INTERNET_PAK_last_nonnull_count', 'USAGE_NUM_INTERNET_PAK_trend', 'REVENUE_INTERNET_PAK_first', 'REVENUE_INTERNET_PAK_last', 'REVENUE_INTERNET_PAK_mean', 'REVENUE_INTERNET_PAK_std', 'REVENUE_INTERNET_PAK_min', 'REVENUE_INTERNET_PAK_max', 'REVENUE_INTERNET_PAK_last_nonnull_count', 'REVENUE_INTERNET_PAK_trend', 'INTERCONNECT_MN_IN_first', 'INTERCONNECT_MN_IN_last', 'INTERCONNECT_MN_IN_mean', 'INTERCONNECT_MN_IN_std', 'INTERCONNECT_MN_IN_min', 'INTERCONNECT_MN_IN_max', 'INTERCONNECT_MN_IN_last_nonnull_count', 'INTERCONNECT_MN_IN_trend', 'INTERCONNECT_MN_OUT_first', 'INTERCONNECT_MN_OUT_last', 'INTERCONNECT_MN_OUT_mean', 'INTERCONNECT_MN_OUT_std', 'INTERCONNECT_MN_OUT_min', 'INTERCONNECT_MN_OUT_max', 'INTERCONNECT_MN_OUT_last_nonnull_count', 'INTERCONNECT_MN_OUT_trend', 'INTERCONNECT_LOC_IN_first', 'INTERCONNECT_LOC_IN_last', 'INTERCONNECT_LOC_IN_mean', 'INTERCONNECT_LOC_IN_std', 'INTERCONNECT_LOC_IN_min', 'INTERCONNECT_LOC_IN_max', 'INTERCONNECT_LOC_IN_last_nonnull_count', 'INTERCONNECT_LOC_IN_trend', 'INTERCONNECT_LOC_OUT_first', 'INTERCONNECT_LOC_OUT_last', 'INTERCONNECT_LOC_OUT_mean', 'INTERCONNECT_LOC_OUT_std', 'INTERCONNECT_LOC_OUT_min', 'INTERCONNECT_LOC_OUT_max', 'INTERCONNECT_LOC_OUT_last_nonnull_count', 'INTERCONNECT_LOC_OUT_trend', 'REVENUE_TOTAL_INTERCONNECT_first', 'REVENUE_TOTAL_INTERCONNECT_last', 'REVENUE_TOTAL_INTERCONNECT_mean', 'REVENUE_TOTAL_INTERCONNECT_std', 'REVENUE_TOTAL_INTERCONNECT_min', 'REVENUE_TOTAL_INTERCONNECT_max', 'REVENUE_TOTAL_INTERCONNECT_last_nonnull_count', 'REVENUE_TOTAL_INTERCONNECT_trend', 'GM_first', 'GM_last', 'GM_mean', 'GM_std', 'GM_min', 'GM_max', 'GM_last_nonnull_count', 'GM_trend', 'REVENUE_TOTAL_first', 'REVENUE_TOTAL_last', 'REVENUE_TOTAL_mean', 'REVENUE_TOTAL_std', 'REVENUE_TOTAL_min', 'REVENUE_TOTAL_max', 'REVENUE_TOTAL_last_nonnull_count', 'REVENUE_TOTAL_trend', 'OTHER_CHARGES_first', 'OTHER_CHARGES_last', 'OTHER_CHARGES_mean', 'OTHER_CHARGES_std', 'OTHER_CHARGES_min', 'OTHER_CHARGES_max', 'OTHER_CHARGES_last_nonnull_count', 'OTHER_CHARGES_trend', 'USAGE_OUT_FREE_OFFNET_VOICE_first', 'USAGE_OUT_FREE_OFFNET_VOICE_last', 'USAGE_OUT_FREE_OFFNET_VOICE_mean', 'USAGE_OUT_FREE_OFFNET_VOICE_std', 'USAGE_OUT_FREE_OFFNET_VOICE_min', 'USAGE_OUT_FREE_OFFNET_VOICE_max', 'USAGE_OUT_FREE_OFFNET_VOICE_last_nonnull_count', 'USAGE_OUT_FREE_OFFNET_VOICE_trend', 'REVENUE_DAILY_ABONKA_first', 'REVENUE_DAILY_ABONKA_last', 'REVENUE_DAILY_ABONKA_mean', 'REVENUE_DAILY_ABONKA_std', 'REVENUE_DAILY_ABONKA_min', 'REVENUE_DAILY_ABONKA_max', 'REVENUE_DAILY_ABONKA_last_nonnull_count', 'REVENUE_DAILY_ABONKA_trend', 'USAGE_DAILY_ABONKA_first', 'USAGE_DAILY_ABONKA_last', 'USAGE_DAILY_ABONKA_mean', 'USAGE_DAILY_ABONKA_std', 'USAGE_DAILY_ABONKA_min', 'USAGE_DAILY_ABONKA_max', 'USAGE_DAILY_ABONKA_last_nonnull_count', 'USAGE_DAILY_ABONKA_trend', 'REVENUE_ROUMING_first', 'REVENUE_ROUMING_last', 'REVENUE_ROUMING_mean', 'REVENUE_ROUMING_std', 'REVENUE_ROUMING_min', 'REVENUE_ROUMING_max', 'REVENUE_ROUMING_last_nonnull_count', 'REVENUE_ROUMING_trend', 'USAGE_NUM_INC_first', 'USAGE_NUM_INC_last', 'USAGE_NUM_INC_mean', 'USAGE_NUM_INC_std', 'USAGE_NUM_INC_min', 'USAGE_NUM_INC_max', 'USAGE_NUM_INC_last_nonnull_count', 'USAGE_NUM_INC_trend', 'REVENUE_OFFNET_O_VOICE_first', 'REVENUE_OFFNET_O_VOICE_last', 'REVENUE_OFFNET_O_VOICE_mean', 'REVENUE_OFFNET_O_VOICE_std', 'REVENUE_OFFNET_O_VOICE_min', 'REVENUE_OFFNET_O_VOICE_max', 'REVENUE_OFFNET_O_VOICE_last_nonnull_count', 'REVENUE_OFFNET_O_VOICE_trend', 'REVENUE_OFFNET_MEGACOM_VOICE_first', 'REVENUE_OFFNET_MEGACOM_VOICE_last', 'REVENUE_OFFNET_MEGACOM_VOICE_mean', 'REVENUE_OFFNET_MEGACOM_VOICE_std', 'REVENUE_OFFNET_MEGACOM_VOICE_min', 'REVENUE_OFFNET_MEGACOM_VOICE_max', 'REVENUE_OFFNET_MEGACOM_VOICE_last_nonnull_count', 'REVENUE_OFFNET_MEGACOM_VOICE_trend', 'ROLY_VOICE_CHARGE_first', 'ROLY_VOICE_CHARGE_last', 'ROLY_VOICE_CHARGE_mean', 'ROLY_VOICE_CHARGE_std', 'ROLY_VOICE_CHARGE_min', 'ROLY_VOICE_CHARGE_max', 'ROLY_VOICE_CHARGE_last_nonnull_count', 'ROLY_VOICE_CHARGE_trend', 'ROLY_DATA_CHARGE_first', 'ROLY_DATA_CHARGE_last', 'ROLY_DATA_CHARGE_mean', 'ROLY_DATA_CHARGE_std', 'ROLY_DATA_CHARGE_min', 'ROLY_DATA_CHARGE_max', 'ROLY_DATA_CHARGE_last_nonnull_count', 'ROLY_DATA_CHARGE_trend', 'ROLY_GLOBAL_first', 'ROLY_GLOBAL_last', 'ROLY_GLOBAL_mean', 'ROLY_GLOBAL_std', 'ROLY_GLOBAL_min', 'ROLY_GLOBAL_max', 'ROLY_GLOBAL_last_nonnull_count', 'ROLY_GLOBAL_trend', 'TOTAL_MOU_first', 'TOTAL_MOU_last', 'TOTAL_MOU_mean', 'TOTAL_MOU_std', 'TOTAL_MOU_min', 'TOTAL_MOU_max', 'TOTAL_MOU_last_nonnull_count', 'TOTAL_MOU_trend', 'LIFETIME_TOTAL_first', 'LIFETIME_TOTAL_last', 'LIFETIME_TOTAL_mean', 'LIFETIME_TOTAL_std', 'LIFETIME_TOTAL_min', 'LIFETIME_TOTAL_max', 'LIFETIME_TOTAL_last_nonnull_count', 'LIFETIME_TOTAL_trend', 'days_from_dt_end_to_date_inactive_first', 'days_from_dt_end_to_date_inactive_last', 'days_from_dt_end_to_date_inactive_mean', 'days_from_dt_end_to_date_inactive_std', 'days_from_dt_end_to_date_inactive_min', 'days_from_dt_end_to_date_inactive_max', 'days_from_dt_end_to_date_inactive_last_nonnull_count', 'days_from_dt_end_to_date_inactive_trend', 'days_from_dt_end_to_date_lad_first', 'days_from_dt_end_to_date_lad_last', 'days_from_dt_end_to_date_lad_mean', 'days_from_dt_end_to_date_lad_std', 'days_from_dt_end_to_date_lad_min', 'days_from_dt_end_to_date_lad_max', 'days_from_dt_end_to_date_lad_last_nonnull_count', 'days_from_dt_end_to_date_lad_trend', 'days_from_dt_end_to_price_change_date_first', 'days_from_dt_end_to_price_change_date_last', 'days_from_dt_end_to_price_change_date_mean', 'days_from_dt_end_to_price_change_date_std', 'days_from_dt_end_to_price_change_date_min', 'days_from_dt_end_to_price_change_date_max', 'days_from_dt_end_to_price_change_date_last_nonnull_count', 'days_from_dt_end_to_price_change_date_trend', 'days_from_dt_end_to_act_date_first', 'days_from_dt_end_to_act_date_last', 'days_from_dt_end_to_act_date_mean', 'days_from_dt_end_to_act_date_std', 'days_from_dt_end_to_act_date_min', 'days_from_dt_end_to_act_date_max', 'days_from_dt_end_to_act_date_last_nonnull_count', 'days_from_dt_end_to_act_date_trend', 'days_from_dt_end_to_date_abonka_first', 'days_from_dt_end_to_date_abonka_last', 'days_from_dt_end_to_date_abonka_mean', 'days_from_dt_end_to_date_abonka_std', 'days_from_dt_end_to_date_abonka_min', 'days_from_dt_end_to_date_abonka_max', 'days_from_dt_end_to_date_abonka_last_nonnull_count', 'days_from_dt_end_to_date_abonka_trend', 'days_from_dt_end_to_date_contract_first', 'days_from_dt_end_to_date_contract_last', 'days_from_dt_end_to_date_contract_mean', 'days_from_dt_end_to_date_contract_std', 'days_from_dt_end_to_date_contract_min', 'days_from_dt_end_to_date_contract_max', 'days_from_dt_end_to_date_contract_last_nonnull_count', 'days_from_dt_end_to_date_contract_trend'] | /tmp/ipykernel_3308355/4151171959.py:2\n",
      "2025-10-30 15:56:55,360 | my_logger - INFO - Numeric wide columns: ['SUBSCRIPTION_FEE_w1', 'SUBSCRIPTION_FEE_w2', 'SUBSCRIPTION_FEE_w3', 'SUBSCRIPTION_FEE_w4', 'BALANCE_END_w1', 'BALANCE_END_w2', 'BALANCE_END_w3', 'BALANCE_END_w4', 'REVENUE_ABONKA_w1', 'REVENUE_ABONKA_w2', 'REVENUE_ABONKA_w3', 'REVENUE_ABONKA_w4', 'USAGE_ABONKA_TP_w1', 'USAGE_ABONKA_TP_w2', 'USAGE_ABONKA_TP_w3', 'USAGE_ABONKA_TP_w4', 'DAYS_WITHOUT_PAYMENT_w1', 'DAYS_WITHOUT_PAYMENT_w2', 'DAYS_WITHOUT_PAYMENT_w3', 'DAYS_WITHOUT_PAYMENT_w4', 'TOTAL_RECHARGE_w1', 'TOTAL_RECHARGE_w2', 'TOTAL_RECHARGE_w3', 'TOTAL_RECHARGE_w4', 'COUNT_RECHARGE_w1', 'COUNT_RECHARGE_w2', 'COUNT_RECHARGE_w3', 'COUNT_RECHARGE_w4', 'USAGE_NUM_OUT_w1', 'USAGE_NUM_OUT_w2', 'USAGE_NUM_OUT_w3', 'USAGE_NUM_OUT_w4', 'USAGE_OUT_ONNET_VOICE_w1', 'USAGE_OUT_ONNET_VOICE_w2', 'USAGE_OUT_ONNET_VOICE_w3', 'USAGE_OUT_ONNET_VOICE_w4', 'USAGE_OUT_OFFNET_VOICE_w1', 'USAGE_OUT_OFFNET_VOICE_w2', 'USAGE_OUT_OFFNET_VOICE_w3', 'USAGE_OUT_OFFNET_VOICE_w4', 'USAGE_OUT_CITY_VOICE_w1', 'USAGE_OUT_CITY_VOICE_w2', 'USAGE_OUT_CITY_VOICE_w3', 'USAGE_OUT_CITY_VOICE_w4', 'USAGE_OUT_INT_VOICE_w1', 'USAGE_OUT_INT_VOICE_w2', 'USAGE_OUT_INT_VOICE_w3', 'USAGE_OUT_INT_VOICE_w4', 'USAGE_OUT_INT_VOICE_RUSSIA_w1', 'USAGE_OUT_INT_VOICE_RUSSIA_w2', 'USAGE_OUT_INT_VOICE_RUSSIA_w3', 'USAGE_OUT_INT_VOICE_RUSSIA_w4', 'USAGE_IN_ONNET_VOICE_w1', 'USAGE_IN_ONNET_VOICE_w2', 'USAGE_IN_ONNET_VOICE_w3', 'USAGE_IN_ONNET_VOICE_w4', 'USAGE_IN_OFFNET_VOICE_w1', 'USAGE_IN_OFFNET_VOICE_w2', 'USAGE_IN_OFFNET_VOICE_w3', 'USAGE_IN_OFFNET_VOICE_w4', 'USAGE_VALUELESS_INTERNET_w1', 'USAGE_VALUELESS_INTERNET_w2', 'USAGE_VALUELESS_INTERNET_w3', 'USAGE_VALUELESS_INTERNET_w4', 'USAGE_INTERNET_w1', 'USAGE_INTERNET_w2', 'USAGE_INTERNET_w3', 'USAGE_INTERNET_w4', 'USAGE_INTERNET_2G_w1', 'USAGE_INTERNET_2G_w2', 'USAGE_INTERNET_2G_w3', 'USAGE_INTERNET_2G_w4', 'USAGE_INTERNET_3G_w1', 'USAGE_INTERNET_3G_w2', 'USAGE_INTERNET_3G_w3', 'USAGE_INTERNET_3G_w4', 'USAGE_INTERNET_LTE_w1', 'USAGE_INTERNET_LTE_w2', 'USAGE_INTERNET_LTE_w3', 'USAGE_INTERNET_LTE_w4', 'USAGE_INTERNET_3G_FREE_w1', 'USAGE_INTERNET_3G_FREE_w2', 'USAGE_INTERNET_3G_FREE_w3', 'USAGE_INTERNET_3G_FREE_w4', 'USAGE_INTERNET_LTE_FREE_w1', 'USAGE_INTERNET_LTE_FREE_w2', 'USAGE_INTERNET_LTE_FREE_w3', 'USAGE_INTERNET_LTE_FREE_w4', 'USAGE_OUT_OFFNET_O_VOICE_w1', 'USAGE_OUT_OFFNET_O_VOICE_w2', 'USAGE_OUT_OFFNET_O_VOICE_w3', 'USAGE_OUT_OFFNET_O_VOICE_w4', 'USAGE_OUT_OFFNET_MEGACOM_VOICE_w1', 'USAGE_OUT_OFFNET_MEGACOM_VOICE_w2', 'USAGE_OUT_OFFNET_MEGACOM_VOICE_w3', 'USAGE_OUT_OFFNET_MEGACOM_VOICE_w4', 'USAGE_IN_OFFNET_O_VOICE_w1', 'USAGE_IN_OFFNET_O_VOICE_w2', 'USAGE_IN_OFFNET_O_VOICE_w3', 'USAGE_IN_OFFNET_O_VOICE_w4', 'USAGE_IN_OFFNET_MEGACOM_VOICE_w1', 'USAGE_IN_OFFNET_MEGACOM_VOICE_w2', 'USAGE_IN_OFFNET_MEGACOM_VOICE_w3', 'USAGE_IN_OFFNET_MEGACOM_VOICE_w4', 'COUNT_SMS_w1', 'COUNT_SMS_w2', 'COUNT_SMS_w3', 'COUNT_SMS_w4', 'REVENUE_VOICE_w1', 'REVENUE_VOICE_w2', 'REVENUE_VOICE_w3', 'REVENUE_VOICE_w4', 'REVENUE_VOICE_TO_SERVICE_w1', 'REVENUE_VOICE_TO_SERVICE_w2', 'REVENUE_VOICE_TO_SERVICE_w3', 'REVENUE_VOICE_TO_SERVICE_w4', 'REVENUE_OUT_ONNET_VOICE_w1', 'REVENUE_OUT_ONNET_VOICE_w2', 'REVENUE_OUT_ONNET_VOICE_w3', 'REVENUE_OUT_ONNET_VOICE_w4', 'REVENUE_OUT_OFFNET_VOICE_w1', 'REVENUE_OUT_OFFNET_VOICE_w2', 'REVENUE_OUT_OFFNET_VOICE_w3', 'REVENUE_OUT_OFFNET_VOICE_w4', 'REVENUE_OUT_CITY_VOICE_w1', 'REVENUE_OUT_CITY_VOICE_w2', 'REVENUE_OUT_CITY_VOICE_w3', 'REVENUE_OUT_CITY_VOICE_w4', 'REVENUE_OUT_INT_VOICE_w1', 'REVENUE_OUT_INT_VOICE_w2', 'REVENUE_OUT_INT_VOICE_w3', 'REVENUE_OUT_INT_VOICE_w4', 'REVENUE_INTERNET_PAYG_w1', 'REVENUE_INTERNET_PAYG_w2', 'REVENUE_INTERNET_PAYG_w3', 'REVENUE_INTERNET_PAYG_w4', 'USAGE_INTERNET_NIGHT_w1', 'USAGE_INTERNET_NIGHT_w2', 'USAGE_INTERNET_NIGHT_w3', 'USAGE_INTERNET_NIGHT_w4', 'USAGE_NUM_INTERNET_PAK_w1', 'USAGE_NUM_INTERNET_PAK_w2', 'USAGE_NUM_INTERNET_PAK_w3', 'USAGE_NUM_INTERNET_PAK_w4', 'REVENUE_INTERNET_PAK_w1', 'REVENUE_INTERNET_PAK_w2', 'REVENUE_INTERNET_PAK_w3', 'REVENUE_INTERNET_PAK_w4', 'INTERCONNECT_MN_IN_w1', 'INTERCONNECT_MN_IN_w2', 'INTERCONNECT_MN_IN_w3', 'INTERCONNECT_MN_IN_w4', 'INTERCONNECT_MN_OUT_w1', 'INTERCONNECT_MN_OUT_w2', 'INTERCONNECT_MN_OUT_w3', 'INTERCONNECT_MN_OUT_w4', 'INTERCONNECT_LOC_IN_w1', 'INTERCONNECT_LOC_IN_w2', 'INTERCONNECT_LOC_IN_w3', 'INTERCONNECT_LOC_IN_w4', 'INTERCONNECT_LOC_OUT_w1', 'INTERCONNECT_LOC_OUT_w2', 'INTERCONNECT_LOC_OUT_w3', 'INTERCONNECT_LOC_OUT_w4', 'REVENUE_TOTAL_INTERCONNECT_w1', 'REVENUE_TOTAL_INTERCONNECT_w2', 'REVENUE_TOTAL_INTERCONNECT_w3', 'REVENUE_TOTAL_INTERCONNECT_w4', 'GM_w1', 'GM_w2', 'GM_w3', 'GM_w4', 'REVENUE_TOTAL_w1', 'REVENUE_TOTAL_w2', 'REVENUE_TOTAL_w3', 'REVENUE_TOTAL_w4', 'OTHER_CHARGES_w1', 'OTHER_CHARGES_w2', 'OTHER_CHARGES_w3', 'OTHER_CHARGES_w4', 'USAGE_OUT_FREE_OFFNET_VOICE_w1', 'USAGE_OUT_FREE_OFFNET_VOICE_w2', 'USAGE_OUT_FREE_OFFNET_VOICE_w3', 'USAGE_OUT_FREE_OFFNET_VOICE_w4', 'REVENUE_DAILY_ABONKA_w1', 'REVENUE_DAILY_ABONKA_w2', 'REVENUE_DAILY_ABONKA_w3', 'REVENUE_DAILY_ABONKA_w4', 'USAGE_DAILY_ABONKA_w1', 'USAGE_DAILY_ABONKA_w2', 'USAGE_DAILY_ABONKA_w3', 'USAGE_DAILY_ABONKA_w4', 'REVENUE_ROUMING_w1', 'REVENUE_ROUMING_w2', 'REVENUE_ROUMING_w3', 'REVENUE_ROUMING_w4', 'USAGE_NUM_INC_w1', 'USAGE_NUM_INC_w2', 'USAGE_NUM_INC_w3', 'USAGE_NUM_INC_w4', 'REVENUE_OFFNET_O_VOICE_w1', 'REVENUE_OFFNET_O_VOICE_w2', 'REVENUE_OFFNET_O_VOICE_w3', 'REVENUE_OFFNET_O_VOICE_w4', 'REVENUE_OFFNET_MEGACOM_VOICE_w1', 'REVENUE_OFFNET_MEGACOM_VOICE_w2', 'REVENUE_OFFNET_MEGACOM_VOICE_w3', 'REVENUE_OFFNET_MEGACOM_VOICE_w4', 'ROLY_VOICE_CHARGE_w1', 'ROLY_VOICE_CHARGE_w2', 'ROLY_VOICE_CHARGE_w3', 'ROLY_VOICE_CHARGE_w4', 'ROLY_DATA_CHARGE_w1', 'ROLY_DATA_CHARGE_w2', 'ROLY_DATA_CHARGE_w3', 'ROLY_DATA_CHARGE_w4', 'ROLY_GLOBAL_w1', 'ROLY_GLOBAL_w2', 'ROLY_GLOBAL_w3', 'ROLY_GLOBAL_w4', 'TOTAL_MOU_w1', 'TOTAL_MOU_w2', 'TOTAL_MOU_w3', 'TOTAL_MOU_w4', 'LIFETIME_TOTAL_w1', 'LIFETIME_TOTAL_w2', 'LIFETIME_TOTAL_w3', 'LIFETIME_TOTAL_w4', 'days_from_dt_end_to_date_inactive_w1', 'days_from_dt_end_to_date_inactive_w2', 'days_from_dt_end_to_date_inactive_w3', 'days_from_dt_end_to_date_inactive_w4', 'days_from_dt_end_to_date_lad_w1', 'days_from_dt_end_to_date_lad_w2', 'days_from_dt_end_to_date_lad_w3', 'days_from_dt_end_to_date_lad_w4', 'days_from_dt_end_to_price_change_date_w1', 'days_from_dt_end_to_price_change_date_w2', 'days_from_dt_end_to_price_change_date_w3', 'days_from_dt_end_to_price_change_date_w4', 'days_from_dt_end_to_act_date_w1', 'days_from_dt_end_to_act_date_w2', 'days_from_dt_end_to_act_date_w3', 'days_from_dt_end_to_act_date_w4', 'days_from_dt_end_to_date_abonka_w1', 'days_from_dt_end_to_date_abonka_w2', 'days_from_dt_end_to_date_abonka_w3', 'days_from_dt_end_to_date_abonka_w4', 'days_from_dt_end_to_date_contract_w1', 'days_from_dt_end_to_date_contract_w2', 'days_from_dt_end_to_date_contract_w3', 'days_from_dt_end_to_date_contract_w4'] | /tmp/ipykernel_3308355/4151171959.py:3\n",
      "2025-10-30 15:56:55,361 | my_logger - INFO - Numeric features table shape: (14120, 504) | /tmp/ipykernel_3308355/4151171959.py:4\n"
     ]
    }
   ],
   "source": [
    "num_tab, num_tab_columns, num_wide_columns = make_num_features_from_wide(num_wide, numeric_cols, COUNT_WEEKS)\n",
    "logger.info(f\"Created numeric features: {num_tab_columns}\")\n",
    "logger.info(f\"Numeric wide columns: {num_wide_columns}\")\n",
    "logger.info(f\"Numeric features table shape: {num_tab.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b81e6b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 15:56:55,669 | my_logger - INFO - After joining numeric features, shape: (14120, 253) | /tmp/ipykernel_3308355/1463284494.py:2\n",
      "2025-10-30 15:56:55,983 | my_logger - INFO - After joining wide categorical features, shape: (14120, 369) | /tmp/ipykernel_3308355/1463284494.py:4\n",
      "2025-10-30 15:56:56,337 | my_logger - INFO - After joining numeric features, shape: (14120, 873) | /tmp/ipykernel_3308355/1463284494.py:6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 15:56:56,739 | my_logger - INFO - After joining categorical features, shape: (14120, 960) | /tmp/ipykernel_3308355/1463284494.py:8\n",
      "2025-10-30 15:56:57,098 | my_logger - INFO - After joining categorical _changes features, shape: (14120, 989) | /tmp/ipykernel_3308355/1463284494.py:10\n"
     ]
    }
   ],
   "source": [
    "df_table = tgt.join(num_wide[num_wide_columns], how='inner', on=index_columns)\n",
    "logger.info(f\"After joining numeric features, shape: {df_table.shape}\")\n",
    "df_table = df_table.join(cat_wide[cat_wide_columns], how='inner', on=index_columns)\n",
    "logger.info(f\"After joining wide categorical features, shape: {df_table.shape}\")\n",
    "df_table = df_table.join(num_tab[num_tab_columns], how='inner', on=index_columns)\n",
    "logger.info(f\"After joining numeric features, shape: {df_table.shape}\")\n",
    "df_table = df_table.join(cat_tab[cat_tab_columns], how='inner', on=index_columns)\n",
    "logger.info(f\"After joining categorical features, shape: {df_table.shape}\")\n",
    "df_table = df_table.join(cat_tab[num_feats], how='inner', on=index_columns)\n",
    "logger.info(f\"After joining categorical _changes features, shape: {df_table.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16e8a5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 15:56:57,904 | my_logger - INFO - Features table saved to /data/aturov/scoring/data/processed/features_table_4_20_60_2025-10-30.parquet | /tmp/ipykernel_3308355/2801462665.py:2\n"
     ]
    }
   ],
   "source": [
    "df_table.to_parquet(config.environment.data_processed_path / f'{NAME_DATAFRAME_TABLE}_{CURRENT_DATE}.parquet', index=True)\n",
    "logger.info(f\"Features table saved to {config.environment.data_processed_path / f'{NAME_DATAFRAME_TABLE}_{CURRENT_DATE}.parquet'}\")\n",
    "# Сохранение категориальных признаков и числовых в json\n",
    "num_columns = num_wide_columns+num_tab_columns + num_feats\n",
    "cat_columns = cat_wide_columns+ cat_tab_columns\n",
    "with open(config.environment.data_processed_path / f'{NAME_DATAFRAME_TABLE}_num_columns_{CURRENT_DATE}.json', 'w') as f:\n",
    "    json.dump(num_columns, f)\n",
    "with open(config.environment.data_processed_path / f'{NAME_DATAFRAME_TABLE}_cat_columns_{CURRENT_DATE}.json', 'w') as f:\n",
    "    json.dump(cat_columns, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc14310e",
   "metadata": {},
   "source": [
    "### Добавим фичи из банковских данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a392044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_full_years(df_part):\n",
    "    # Робастный расчёт возраста (полные годы на дату открытия)\n",
    "    df_part['date_open'] = pd.to_datetime(df_part['date_open'], errors='coerce')\n",
    "    df_part['date_birth'] = pd.to_datetime(df_part['date_birth'], errors='coerce')\n",
    "\n",
    "    # опционально: сохраняем дни для отладки/аналитики\n",
    "    df_part['age_days'] = (df_part['date_open'] - df_part['date_birth']).dt.days\n",
    "\n",
    "    # вычисляем полные годы (векторно, без apply)\n",
    "    mask = df_part['date_open'].notna() & df_part['date_birth'].notna()\n",
    "    y_open = df_part.loc[mask, 'date_open'].dt.year\n",
    "    m_open = df_part.loc[mask, 'date_open'].dt.month\n",
    "    d_open = df_part.loc[mask, 'date_open'].dt.day\n",
    "    y_birth = df_part.loc[mask, 'date_birth'].dt.year\n",
    "    m_birth = df_part.loc[mask, 'date_birth'].dt.month\n",
    "    d_birth = df_part.loc[mask, 'date_birth'].dt.day\n",
    "\n",
    "    age_years = y_open - y_birth - ((m_open < m_birth) | ((m_open == m_birth) & (d_open < d_birth))).astype(int)\n",
    "    df_part.loc[mask, 'age'] = age_years\n",
    "\n",
    "    # nullable integer и фильтрация не реалистичных значений\n",
    "    df_part['age'] = df_part['age'].astype('Int64')\n",
    "    df_part.loc[~df_part['age'].between(0, 120), 'age'] = pd.NA\n",
    "    return df_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24e5691c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 15:56:58,429 | my_logger - INFO - Data shape from ClickHouse: (18631, 23) | /tmp/ipykernel_3308355/4205655774.py:21\n",
      "2025-10-30 15:56:58,450 | my_logger - INFO - Data shape from ClickHouse: (18631, 14) | /tmp/ipykernel_3308355/4205655774.py:30\n",
      "2025-10-30 15:56:58,450 | my_logger - INFO - Data shape from ClickHouse: (18631, 14) | /tmp/ipykernel_3308355/4205655774.py:32\n",
      "/tmp/ipykernel_3308355/4205655774.py:46: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_part[col].fillna(-1, inplace=True)\n",
      "/tmp/ipykernel_3308355/4205655774.py:49: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_part[col].fillna('unknown', inplace=True)\n",
      "/tmp/ipykernel_3308355/4205655774.py:49: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_part[col].fillna('unknown', inplace=True)\n",
      "/tmp/ipykernel_3308355/4205655774.py:49: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_part[col].fillna('unknown', inplace=True)\n",
      "2025-10-30 15:56:58,570 | my_logger - INFO - Data shape from ClickHouse after processing: (18410, 15) | /tmp/ipykernel_3308355/4205655774.py:53\n"
     ]
    }
   ],
   "source": [
    "def make_query(engine):\n",
    "    \"\"\"\n",
    "    Функция для выполнения SQL-запроса к базе данных и получения данных в виде DataFrame.\n",
    "    Параметры:\n",
    "    - engine: SQLAlchemy engine для подключения к базе данных.\n",
    "    Возвращает:\n",
    "    - DataFrame с результатами запроса.\n",
    "    \"\"\"\n",
    "\n",
    "    query = f\"\"\"\n",
    "            SELECT \n",
    "                *\n",
    "            FROM data_science.credits_subs_eldik_clean AS ce\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    data = pd.read_sql(query, engine)\n",
    "\n",
    "    return data\n",
    "df_part = make_query(clickhouse_engine)\n",
    "logger.info(f\"Data shape from ClickHouse: {df_part.shape}\")\n",
    "df_part.drop(columns=['phone_beeline', \n",
    "                      'inn_beeline', 'subs_eff_dt',  'interest_on_credit',\n",
    "                      'match_phone', 'match_inn', 'overdue_max',\n",
    "                        'total_overdue', 'status'], inplace=True)\n",
    "df_part.date_birth = pd.to_datetime(df_part.date_birth, errors='coerce')\n",
    "df_part.date_open = pd.to_datetime(df_part.date_open, errors='coerce')\n",
    "df_part.dropna(subset=['date_open', 'date_birth'], inplace=True)\n",
    "\n",
    "logger.info(f\"Data shape from ClickHouse: {df_part.shape}\")\n",
    "\n",
    "logger.info(f\"Data shape from ClickHouse: {df_part.shape}\")\n",
    "df_part = age_full_years(df_part)\n",
    "df_part.drop(columns=['date_birth'], inplace=True)\n",
    "\n",
    "cat_banking_features = ['sex',  'birthplace', 'marital_status', \n",
    "       #'name_region', 'city', 'street'\n",
    "       ]\n",
    "num_banking_features = ['age', 'age_days', 'prev_credit_count', 'sum_of_prev_credits',\n",
    "                        'contract_length', #'interest_on_credit', \n",
    "                        'summa'\n",
    "                        ]\n",
    "\n",
    "\n",
    "for col in num_banking_features:\n",
    "    df_part[col].fillna(-1, inplace=True)\n",
    "for col in cat_banking_features:\n",
    "    df_part[col] = df_part[col].astype(str)\n",
    "    df_part[col].fillna('unknown', inplace=True)\n",
    "\n",
    "\n",
    "df_part.drop_duplicates(subset=['id_request', 'inn_eldik','id_credit'], inplace=True)\n",
    "logger.info(f\"Data shape from ClickHouse after processing: {df_part.shape}\")\n",
    "df_part.set_index(index_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d648fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 15:56:59,063 | my_logger - INFO - Data shape after joining with features table: (13901, 998) | /tmp/ipykernel_3308355/2143672639.py:2\n"
     ]
    }
   ],
   "source": [
    "df_part = df_part.join(df_table, how='inner', on=index_columns)\n",
    "logger.info(f\"Data shape after joining with features table: {df_part.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af5082d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 15:56:59,864 | my_logger - INFO - Features saved to /data/aturov/scoring/data/processed/features_table_banking_4_20_60_2025-10-30.parquet | /tmp/ipykernel_3308355/2904767515.py:2\n"
     ]
    }
   ],
   "source": [
    "df_part.to_parquet(config.environment.data_processed_path / f'{NAME_DATAFRAME_BANKING}_{CURRENT_DATE}.parquet', index=True)\n",
    "logger.info(f\"Features saved to {config.environment.data_processed_path / f'{NAME_DATAFRAME_BANKING}_{CURRENT_DATE}.parquet'}\")\n",
    "cat_banking_features += cat_tab_columns + cat_wide_columns\n",
    "num_banking_features += num_tab_columns + num_wide_columns\n",
    "# Сохранение категориальных признаков и числовых в json\n",
    "import json\n",
    "num_columns = num_columns + num_banking_features\n",
    "cat_columns = cat_columns + cat_banking_features\n",
    "with open(config.environment.data_processed_path / f'{NAME_DATAFRAME_BANKING}_cat_columns_{CURRENT_DATE}.json', 'w') as f:\n",
    "    json.dump(cat_columns, f)\n",
    "with open(config.environment.data_processed_path / f'{NAME_DATAFRAME_BANKING}_num_columns_{CURRENT_DATE}.json', 'w') as f:\n",
    "    json.dump(num_columns, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
